{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f75301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Images: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "MNIST Lables: [5 0 4 ... 5 6 8]\n",
      "Digit 0\n",
      "Mnist Images 5923\n",
      "Mnist Labels 5923\n",
      "Digit 1\n",
      "Mnist Images 6742\n",
      "Mnist Labels 6742\n",
      "Digit 2\n",
      "Mnist Images 5958\n",
      "Mnist Labels 5958\n",
      "Digit 3\n",
      "Mnist Images 6131\n",
      "Mnist Labels 6131\n",
      "Digit 4\n",
      "Mnist Images 5842\n",
      "Mnist Labels 5842\n",
      "Digit 5\n",
      "Mnist Images 5421\n",
      "Mnist Labels 5421\n",
      "Digit 6\n",
      "Mnist Images 5918\n",
      "Mnist Labels 5918\n",
      "Digit 7\n",
      "Mnist Images 6265\n",
      "Mnist Labels 6265\n",
      "Digit 8\n",
      "Mnist Images 5851\n",
      "Mnist Labels 5851\n",
      "Digit 9\n",
      "Mnist Images 5949\n",
      "Mnist Labels 5949\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshay Patil\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAABqCAYAAAA7iicOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOZElEQVR4nO3de3CU1RnH8efdTULCxeWWCwwCaqtVBjpCaWOtBGhpRW2rQm2xF4hjh2mlCkrrtIpS2hGZTir9Qy2dTuO0Y6kaL9iCF2QIMHa14AWLaFtawEuEKOZGgODunv5TUqP4nN2zJ5t3N9/PTGaY/N68e/iR7O4zL3lPYIwxAgAAAAAeRfp6AQAAAAAKD4MGAAAAAO8YNAAAAAB4x6ABAAAAwDsGDQAAAADeMWgAAAAA8I5BAwAAAIB3DBoAAAAAvGPQAAAAAOCdl0HjnnvukSAIuj9KS0ulqqpKZsyYIStXrpTm5uYexy9fvlyCIHB6rMbGRgmCQBobG7s/t2HDBlm+fHlG51m9erVcfvnlctppp0kQBDJ9+nSn9WSD3tzRnTu6c0d37ujODb25ozt3dOeO7j7AeFBfX29ExNTX15t4PG62bt1qGhoazOLFi00sFjPDhw83Gzdu7D7+9ddfN/F43Omx2traTDweN21tbd2fu+aaa0ymf5WzzjrLTJ482Vx11VWmvLzc1NTUOK0nG/Tmju7c0Z07unNHd27ozR3duaM7d3TXk9dBY/v27R/K9u/fb0499VQzZMgQc+DAAR8P9yEupSaTye4/T5gwoU9fQOgtc3Tnju7c0Z07unNDb+7ozh3duaO7nnr9dzTGjh0rdXV10tHRIWvWrBGRk18m6urqkhtuuEGqqqpk4MCBMm3aNHnuuedk/PjxsmDBgu7jPniZaMGCBXLnnXeKiPS4VLVv3z51XZFIuH89hd7c0Z07unNHd+7ozg29uaM7d3Tnrj92l5N/lYsuukii0ahs3br1I4+pra2V1atXS21traxbt07mzJkjl112mbS2tqrnXrZsmcydO1dEROLxePfHqFGjROT//4Dv//9r+YLe3NGdO7pzR3fu6M4NvbmjO3d0566/dVeUiwcZNGiQjBw5Upqamk6a7969W9auXSs33nijrFy5UkREZs2aJZWVlTJv3jz13GeccYZUVlaKiEh1dfWH8kgkItFo1PkXbfoSvbmjO3d0547u3NGdG3pzR3fu6M5df+suZ9eZjDEfmW3ZskVERK644ooen587d64UFWU3C91yyy2SSCSkpqYmq/P0FXpzR3fu6M4d3bmjOzf05o7u3NGdu/7UXU4Gjc7OTjl06JCMHj36pPmhQ4dERLqnsBOKiopkxIgRvb6+sKI3d3Tnju7c0Z07unNDb+7ozh3duetv3eVk0Fi/fr0kk8mPvC/vieIOHjzY4/OJRKK78P6I3tzRnTu6c0d37ujODb25ozt3dOeuv3XX64PGa6+9JkuXLpVYLCYLFy486THTpk0TEZH77ruvx+cbGhokkUhYH2PAgAEiInL06NEsVxse9OaO7tzRnTu6c0d3bujNHd25ozt3/bE7r78MvmvXLkkkEpJIJKS5uVm2bdsm9fX1Eo1G5eGHH5by8vKTft2ECRNk3rx5UldXJ9FoVGbOnCkvv/yy1NXVSSwWs952a+LEiSIismrVKpk9e7ZEo1GZNGmSlJSUyIoVK2TFihWyadOmHv8nbceOHd23+2pvbxdjjDQ0NIiIyNSpU2XcuHEeGkkPvbmjO3d0547u3NGdG3pzR3fu6M4d3f2Pj804TmxOcuKjpKTEVFRUmJqaGnPbbbeZ5ubmHsffeuutH9pM5NixY+b66683FRUVprS01FRXV5t4PG5isZhZsmRJ93GbN282ImI2b97c/bmuri5z9dVXm/LychMEgRERs3fv3h6P9f7jjTFm/vz5Pdb8/o/6+noftVjRmzu6c0d37ujOHd25oTd3dOeO7tzRXU9eBo3e8vTTTxsRMffee29fLyWv0Js7unNHd+7ozh3duaE3d3Tnju7c5Wt3gTHKPbZyaOPGjRKPx2XKlClSVlYmO3fulNtvv11isZi89NJLUlpa2tdLDCV6c0d37ujOHd25ozs39OaO7tzRnbuC6q6vJ50TnnnmGXP++eebYcOGmaKiIlNVVWXmz59vmpqa+nppoUZv7ujOHd25ozt3dOeG3tzRnTu6c1dI3YXmigYAAACAwpGzncEBAAAA9B8MGgAAAAC8Y9AAAAAA4B2DBgAAAADv0t4ZPAi8biLeSwJLnv3vvRtj3/79g3LRXRBYbnVmWbeRzP9emQprd/kg0+5y01vv/7xlq3C/5/rvc51N+SlT1fydjp1qbszxrNeQr92F4Wc6f7vTFRUNV/NEoiWNs+j9F2p3uRDG11jb90xZyQg17zjyL5/LOal0euOKBgAAAADvGDQAAAAAeMegAQAAAMA7Bg0AAAAA3jFoAAAAAPCOQQMAAACAdwwaAAAAALzLqxsolxRXqHn8s19X8wtfeMb6GG+3b89oTblhu7e5yF+mXqvmTx3U99m4Y/+KjFZUSGbGlqj50Kje3UPvrvS5nJCIWo84MPebav7VjYPU/Nm2uzNaUX9SFB2q5pura9X84hc2WB+j/cg/MllSaEQig9X8reUT1PyTPztTzV9uuTfjNeWLsgFj1HzT1Llq/rm//lbNU6nDGa+pUIweOk3N33jkC2o+/JL7rY/RenhXRmvKBeseXiIyKaa/Nxtp9P0g3oy8oeavtjxgXUMY9nV6vwHFVdZjWhdNV/OgRH9vWLZqr5rnYv80Ea5oAAAAAOgFDBoAAAAAvGPQAAAAAOAdgwYAAAAA7xg0AAAAAHjHoAEAAADAOwYNAAAAAN6FbB8N/Z7AvzxzoZqfu7BNzTtrmzNeUb44Pdau5l+OJtX8jv22fRP0r89nqz6p/93GVer38H4onVt4552U9Yjym85V8wePv6TmYx7NaEEFRn+u+/WERWp+3sMT1Tw59pFMFxQK0egp1mN+PH6xmptr9f0M3vnpskyWVFBOHVit5lMbL1Hz4rI/qXlXQe+job9GNn7mE2qe2PpPNW/r3JPxisLAmGPWY5pSu9X8+S1XqvkTC/W3qhdvL7auwZjj1mNyqbbiu9Zjin5Ro+aHvvF7NTched/GFQ0AAAAA3jFoAAAAAPCOQQMAAACAdwwaAAAAALxj0AAAAADgHYMGAAAAAO8YNAAAAAB4x6ABAAAAwLtQbdg3fthsNf/eA1VqPuu8TjU/0rU/4zWFg7Eecf++CjW/+UcH1Tzy7AA1T6WOWNeQr/5zeJCaT55v2ejnAX3ztXT+/cLHvmaz7e9qXrV2jn6CQfdkvYZ8NXboLDWf//x0NV80bouadx7bm+mScmJAsf4cvm7yt63nGFTUoebR9U9mtKb+ZHrZ2Vl9fTJl35ytUF049Ho1P239l9R8Yvlv1Dydje/y1aJK/b1daqK+Ael3XvmDmodtM750rGn6hfWYMefo7y1mVIzwtZxexRUNAAAAAN4xaAAAAADwjkEDAAAAgHcMGgAAAAC8Y9AAAAAA4B2DBgAAAADvGDQAAAAAeJezfTSi0VOsx+xZMUrNd37/VTXf1HZXRmsqJO926XkwtlzNI5FSNS/kfTRe7ShWczP7AjUP5Lf610si4zXlhZS+z0Xw5ps5Wki4RCKDrcfsuU7/eey8+ndqvqapIaM1hUXXewfU/KLtd1vPEbU8V7Vu/IKaDwiGWB7Bti+OSL7u8XJeeUrNg7v+qOaJpL6HSb5K5/3JhrX660TjBY+p+astD2a0pnwRpPE28uZl76r58R/qr6Gdx5szWlM+SGfflHEDk2r+Qov+WvPxYZep+Z6WR61r8PH+hSsaAAAAALxj0AAAAADgHYMGAAAAAO8YNAAAAAB4x6ABAAAAwDsGDQAAAADeMWgAAAAA8C5n+2h8bfgPrMeY781Q8wtvuslyBv2ew4VsR5t+n+rk5y9V89OH6HuU/LMlP+/bn44NzW+r+U8qpql5YLmvv0kdznhN+cAc03/eIq/8O0crCZfaysXWY4Jl09X89GFL1dyY4xmsKDyKokPV/K5zFlnPccWU/6h5yRj9ZW395I+p+ezn7a8jb7Rush4TRsNL9Hvid+yw3dtf34cjX11VeZ31mNS556j5V1682XKGwnx/UlI80npM6spL1XzOtU+q+eiBU9T8313p7NkUrr1v0tlv6colrWqe+saX1fz76/T3HgO/VWFdw7HjTdZjbLiiAQAAAMA7Bg0AAAAA3jFoAAAAAPCOQQMAAACAdwwaAAAAALxj0AAAAADgHYMGAAAAAO887qMRqOlNE1usZ9gzW9+robn9bxmtqD/5W+ef1Nz8XO9/RKrS53Lyyj55Uc2jj8XU3Bj93vSFatu6cjW/YOkXLWeos+Rhve+8/lx37dn257o/Vz+l5u927MxoRfliUOkoNbft8yAi8ul1+h4iTfc9q+bJVJeaH+16y7qGcNK/L0VEJpYfUvMHXzzdcoZw7UXgy6Vj9O8JEZHkrx5V886j+30tJ6+cMVjf/0xEJLJGf3/yeLuepwpwL6rS4hHWY1ofb1Pz3b95Qs3n/P2vau5jj4x0cEUDAAAAgHcMGgAAAAC8Y9AAAAAA4B2DBgAAAADvGDQAAAAAeMegAQAAAMA7Bg0AAAAA3jFoAAAAAPAuMMaktQNPEGS3t9+wwZOsx6TMe2re1vlKVmvwwWVztmy78yEaPUXNk8lOyxmy3zwtrN0FQamajxs6U833tWzwuZyTyrS7XPRWUlyh5qMHf0rNw9ibSG6e6450Nat513sHslqDD33z82rfdC4fNo0L63Od7XszDN+XfdFdJDIw68dIJtuzWoMPfdGd7b2FiMiQsrFq3np4V1Zr8CGMr7HpPR9qev+5Mp3euKIBAAAAwDsGDQAAAADeMWgAAAAA8I5BAwAAAIB3DBoAAAAAvGPQAAAAAOAdgwYAAAAA79LeRwMAAAAA0sUVDQAAAADeMWgAAAAA8I5BAwAAAIB3DBoAAAAAvGPQAAAAAOAdgwYAAAAA7xg0AAAAAHjHoAEAAADAOwYNAAAAAN79F02ds3Iia9tuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit zero [[0.00000000e+00 0.00000000e+00 5.02834929e-05 ... 5.04943733e-03\n",
      "  2.54318312e-06 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.64488586e-06 ... 2.44551014e-02\n",
      "  2.39711887e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.95373525e-05 ... 5.52719367e-02\n",
      "  4.29082453e-06 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 4.33822537e-04\n",
      "  1.93436207e-08 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.30475596e-07 ... 1.49584173e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.45087318e-07 ... 5.72852573e-04\n",
      "  3.43886591e-08 0.00000000e+00]]\n",
      "Digit one [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.41505166e-07\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.26532695e-02 ... 2.13737652e+00\n",
      "  3.00926886e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.68420508e-04 ... 4.41504330e-01\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.00300256e-08\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "Digit two [[0.00000000e+00 0.00000000e+00 7.84932524e-05 ... 1.93221215e-07\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 3.25746758e-05 4.92325911e-02 ... 3.42236706e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.62609041e-07 3.21443234e-03 1.93140111e-01 ... 3.97737330e-01\n",
      "  5.90331773e-02 6.99977939e-04]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 1.43086426e-06 ... 1.06666198e-07\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 9.93283517e-05 ... 9.31359517e-09\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 7.52050682e-06 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "Digit three [[0.00000000e+00 4.86122711e-05 3.32059775e-02 ... 9.35560252e-02\n",
      "  1.13816745e-04 0.00000000e+00]\n",
      " [0.00000000e+00 4.96804584e-05 2.26196127e-01 ... 3.13713403e-02\n",
      "  1.10288268e-06 0.00000000e+00]\n",
      " [5.01702814e-02 7.02311634e-01 1.25197241e+00 ... 5.88460693e+00\n",
      "  1.06375576e-01 3.70588782e-05]\n",
      " ...\n",
      " [0.00000000e+00 7.62586721e-05 3.64739374e-02 ... 2.33131980e-01\n",
      "  1.68717635e-04 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.34960619e-04 ... 7.19507710e-02\n",
      "  1.38075070e-04 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.64940835e-02 ... 1.20641614e-01\n",
      "  1.40276985e-05 0.00000000e+00]]\n",
      "Digit four [[1.64603484e-03 2.43286900e-02 7.43828497e-04 ... 2.08716786e+01\n",
      "  4.21309116e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.50667597e-06\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.06706981e-02 1.32550191e-01 2.37977170e-03 ... 3.74591374e+00\n",
      "  2.17130147e+00 8.31401541e-03]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 1.62393152e-06 ... 9.78003279e-06\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.28332441e-06 ... 1.13513252e-07\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.94360632e-07\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "Digit fifth [[3.51050895e-08 4.58186454e-04 2.54365747e-02 ... 4.58378766e-03\n",
      "  3.56382968e-06 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 7.26767195e-08\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.07764815e-08\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.89206287e-04\n",
      "  5.37322798e-08 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 8.84818701e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.10094258e-06 ... 9.80972363e-01\n",
      "  4.59710735e-04 0.00000000e+00]]\n",
      "Digit Sixth [[0.00000000e+00 0.00000000e+00 2.94352617e-03 ... 4.27524229e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.12722470e-05 ... 6.61074369e-02\n",
      "  5.02084118e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 6.08880560e-03 ... 8.77544414e-03\n",
      "  9.16593011e-06 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.70465970e-07\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 7.45087614e-08 ... 5.26034613e-06\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.94838265e-07 ... 1.41261104e-04\n",
      "  1.50450383e-08 0.00000000e+00]]\n",
      "Digit seventh [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.02764401e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.71860149e-06 3.07452546e-04 2.07655901e-03 ... 8.34936285e-01\n",
      "  9.47159335e-05 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 1.66129921e-07 ... 1.57614687e-08\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.23198636e-04 ... 2.56667369e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.57914943e-08\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "Digit eigth [[0.00000000e+00 0.00000000e+00 7.01546876e-05 ... 9.28191792e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.14045827e-04 ... 2.10000921e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.14806231e-04 ... 1.52238990e-02\n",
      "  5.37322798e-08 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 1.02555932e-06 ... 2.93414957e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.07464560e-08 5.31397729e-03 ... 6.81216945e-02\n",
      "  1.37554636e-07 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 9.94036385e-07 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "Digit nineth [[0.00000000e+00 0.00000000e+00 1.10568945e-05 ... 2.83852284e+01\n",
      "  3.83617762e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.43640796e-07 ... 1.68365866e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 7.02101790e-08 ... 3.63231341e-06\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 6.76002895e-07 ... 2.82310827e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.42303528e-07 ... 7.97396881e-07\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.55453770e-06 ... 3.90075711e-07\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Quantum Amplitude Encoding\n",
    "\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_MNIST_dataset_Images(file_path): # for now trial on mnist dataset\n",
    "    images = idx2numpy.convert_from_file(file_path)\n",
    "    images = images.reshape(images.shape[0], -1)\n",
    "    images = images.astype(np.float64)\n",
    "    return images\n",
    "\n",
    "def load_MNIST_dataset_Labels(file_path):\n",
    "    labels = idx2numpy.convert_from_file(file_path)\n",
    "    return labels\n",
    "\n",
    "# Download the MNIST Dataset from Kaggle and Change this directory to your Directory\n",
    "train_images_file = r\"D:\\Downloads Now\\archive\\train-images.idx3-ubyte\"\n",
    "train_labels_file = r\"D:Downloads Now\\archive\\train-labels.idx1-ubyte\"\n",
    "\n",
    "mnist_images = load_MNIST_dataset_Images(train_images_file)\n",
    "mnist_labels = load_MNIST_dataset_Labels(train_labels_file)\n",
    "\n",
    "print('MNIST Images:', mnist_images)\n",
    "print('MNIST Lables:', mnist_labels)\n",
    "\n",
    "\n",
    "# Separating the load dataset into train labels and the train digits \n",
    "\n",
    "def separate_images_and_labels(images , labels , digit):\n",
    "    digit_images = []\n",
    "    digit_labels = []\n",
    "    for image,label in zip(images,labels):\n",
    "        if label == digit:\n",
    "            digit_images.append(image)\n",
    "            digit_labels.append(label)\n",
    "    return digit_images,digit_labels\n",
    "\n",
    "digits_to_separate = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "digit_image = {}\n",
    "digit_label = {}\n",
    "\n",
    "for digit in digits_to_separate:\n",
    "    digit_image[digit], digit_label[digit] = separate_images_and_labels(mnist_images,mnist_labels,digit)\n",
    "\n",
    "for digit in digits_to_separate:\n",
    "    print('Digit', digit)\n",
    "    print('Mnist Images', len(digit_image[digit]))\n",
    "    print('Mnist Labels', len(digit_label[digit]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 10 \n",
    "\n",
    "for digit in digit_image.keys():\n",
    "    sample_images = digit_image[digit][:num_samples]\n",
    "    plt.figure(figsize = (10,2))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(sample_images[i].reshape(28,28), cmap = 'gray')\n",
    "        plt.title(f\"Digit: {digit}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Resize the image to a higher accuracy and match the dimensions of the qubits\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "def resize_images(images, new_size = (8,8)):\n",
    "    resized_images = []\n",
    "    for image in images:\n",
    "        resized_image = resize(image.reshape(28,28), new_size)\n",
    "        resized_images.append(resized_image.flatten())\n",
    "    return resized_images\n",
    "\n",
    "new_size = (8,8)\n",
    "\n",
    "resized_digit_images = {}\n",
    "for digit in digit_image.keys():\n",
    "    resized_digit_images[digit] = resize_images(digit_image[digit], new_size = new_size)\n",
    "\n",
    "for digit in digit_image.keys():\n",
    "    sample_images = resized_digit_images[digit][:num_samples]\n",
    "    plt.figure(figsize =(10,2))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i+1)\n",
    "        plt.imshow(sample_images[i].reshape(new_size), cmap = 'magma')\n",
    "        plt.title(f\"Digit:{digit}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Converting all the digits in to arrays\n",
    "\n",
    "Digit_zero = []\n",
    "\n",
    "for image in resized_digit_images[0]:\n",
    "    Digit_zero_ = image\n",
    "    Digit_zero.append(Digit_zero_)\n",
    "\n",
    "Digit_zero = np.array(Digit_zero)\n",
    "\n",
    "print('Digit zero',Digit_zero)\n",
    "\n",
    "Digit_one = []\n",
    "\n",
    "for image in resized_digit_images[1]:\n",
    "    Digit_one_ = image\n",
    "    Digit_one.append(Digit_one_)\n",
    "\n",
    "Digit_one = np.array(Digit_one)\n",
    "\n",
    "print('Digit one',Digit_one)\n",
    "\n",
    "Digit_two = []\n",
    "\n",
    "for image in resized_digit_images[2]:\n",
    "    Digit_two_ = image\n",
    "    Digit_two.append(Digit_two_)\n",
    "\n",
    "Digit_two = np.array(Digit_two)\n",
    "\n",
    "print('Digit two',Digit_two)\n",
    "\n",
    "Digit_three = []\n",
    "\n",
    "for image in resized_digit_images[3]:\n",
    "    Digit_three_ = image\n",
    "    Digit_three.append(Digit_three_)\n",
    "\n",
    "Digit_three = np.array(Digit_three)\n",
    "\n",
    "print('Digit three',Digit_three)\n",
    "\n",
    "Digit_four = []\n",
    "\n",
    "for image in resized_digit_images[4]:\n",
    "    Digit_four_ = image\n",
    "    Digit_four.append(Digit_four_)\n",
    "\n",
    "Digit_four = np.array(Digit_four)\n",
    "\n",
    "print('Digit four',Digit_four)\n",
    "\n",
    "Digit_fifth = []\n",
    "\n",
    "for image in resized_digit_images[5]:\n",
    "    Digit_fifth_ = image\n",
    "    Digit_fifth.append(Digit_fifth_)\n",
    "\n",
    "Digit_fifth = np.array(Digit_fifth)\n",
    "\n",
    "print('Digit fifth',Digit_fifth)\n",
    "\n",
    "Digit_sixth = []\n",
    "\n",
    "for image in resized_digit_images[6]:\n",
    "    Digit_sixth_ = image\n",
    "    Digit_sixth.append(Digit_sixth_)\n",
    "\n",
    "Digit_sixth = np.array(Digit_sixth)\n",
    "\n",
    "print('Digit Sixth',Digit_sixth)\n",
    "\n",
    "Digit_seventh = []\n",
    "\n",
    "for image in resized_digit_images[7]:\n",
    "    Digit_seventh_ = image\n",
    "    Digit_seventh.append(Digit_seventh_)\n",
    "\n",
    "Digit_seventh = np.array(Digit_seventh)\n",
    "\n",
    "print('Digit seventh',Digit_seventh)\n",
    "\n",
    "Digit_eigth = []\n",
    "\n",
    "for image in resized_digit_images[8]:\n",
    "    Digit_eigth_ = image\n",
    "    Digit_eigth.append(Digit_eigth_)\n",
    "\n",
    "Digit_eigth = np.array(Digit_eigth)\n",
    "\n",
    "print('Digit eigth',Digit_eigth)\n",
    "\n",
    "Digit_nineth = []\n",
    "\n",
    "for image in resized_digit_images[9]:\n",
    "    Digit_nineth_ = image\n",
    "    Digit_nineth.append(Digit_nineth_)\n",
    "\n",
    "Digit_nineth = np.array(Digit_nineth)\n",
    "\n",
    "print('Digit nineth',Digit_nineth)\n",
    "\n",
    "# Digit zero to Nine grey scale matrix conversion by reshaping a row array into a (N,N) size\n",
    "\n",
    "# Quantum Amplitude Encoding of MNIST Dataset\n",
    "\n",
    "N = 8\n",
    "\n",
    "Digit_zero_ = []\n",
    "\n",
    "for i in range(len(Digit_zero)):\n",
    "    Digit_zero_.append((Digit_zero[i].reshape(N**2))/np.linalg.norm(Digit_zero[i]))\n",
    "\n",
    "Digit_one_ = []\n",
    "\n",
    "for i in range(len(Digit_one)):\n",
    "    Digit_one_.append((Digit_one[i].reshape(N**2))/np.linalg.norm(Digit_one[i]))\n",
    "\n",
    "Digit_two_ = []\n",
    "\n",
    "for i in range(len(Digit_two)):\n",
    "    Digit_two_.append((Digit_two[i].reshape(N**2))/np.linalg.norm(Digit_two[i]))\n",
    "\n",
    "Digit_three_ = []\n",
    "\n",
    "for i in range(len(Digit_three)):\n",
    "    Digit_three_.append((Digit_three[i].reshape(N**2))/np.linalg.norm(Digit_three[i]))\n",
    "\n",
    "Digit_four_ = []\n",
    "\n",
    "for i in range(len(Digit_four)):\n",
    "    Digit_four_.append((Digit_four[i].reshape(N**2))/np.linalg.norm(Digit_four[i]))\n",
    "\n",
    "Digit_five_ = []\n",
    "\n",
    "for i in range(len(Digit_fifth)):\n",
    "    Digit_five_.append((Digit_fifth[i].reshape(N**2))/np.linalg.norm(Digit_fifth[i]))\n",
    "\n",
    "Digit_sixth_ = []\n",
    "\n",
    "for i in range(len(Digit_sixth)):\n",
    "    Digit_sixth_.append((Digit_sixth[i].reshape(N**2))/np.linalg.norm(Digit_sixth[i]))\n",
    "\n",
    "Digit_seventh_ = []\n",
    "\n",
    "for i in range(len(Digit_seventh)):\n",
    "    Digit_seventh_.append((Digit_seventh[i].reshape(N**2))/np.linalg.norm(Digit_seventh[i]))\n",
    "\n",
    "Digit_eigth_ = []\n",
    "\n",
    "for i in range(len(Digit_eigth)):\n",
    "    Digit_eigth_.append((Digit_eigth[i].reshape(N**2))/np.linalg.norm(Digit_eigth[i]))\n",
    "\n",
    "Digit_nineth_ = []\n",
    "\n",
    "for i in range(len(Digit_nineth)):\n",
    "    Digit_nineth_.append((Digit_nineth[i].reshape(N**2))/np.linalg.norm(Digit_nineth[i]))\n",
    "\n",
    "\n",
    "Input_state_vector_zero = np.array(Digit_zero_)\n",
    "Input_state_vector_one = np.array(Digit_one_)\n",
    "Input_state_vector_two = np.array(Digit_two_)\n",
    "Input_state_vector_three = np.array(Digit_three_)\n",
    "Input_state_vector_four = np.array(Digit_four_)\n",
    "Input_state_vector_fifth = np.array(Digit_five_)\n",
    "Input_state_vector_sixth = np.array(Digit_sixth_)\n",
    "Input_state_vector_seventh = np.array(Digit_seventh_)\n",
    "Input_state_vector_eigth = np.array(Digit_eigth_)\n",
    "Input_state_vector_nineth = np.array(Digit_nineth_)\n",
    "\n",
    "input_state_vectors = []\n",
    "\n",
    "input_state_vectors.append(Input_state_vector_zero)\n",
    "input_state_vectors.append(Input_state_vector_one)\n",
    "input_state_vectors.append(Input_state_vector_two)\n",
    "input_state_vectors.append(Input_state_vector_three)\n",
    "input_state_vectors.append(Input_state_vector_four)\n",
    "input_state_vectors.append(Input_state_vector_fifth)\n",
    "input_state_vectors.append(Input_state_vector_sixth)\n",
    "input_state_vectors.append(Input_state_vector_seventh)\n",
    "input_state_vectors.append(Input_state_vector_eigth)\n",
    "input_state_vectors.append(Input_state_vector_nineth)\n",
    "\n",
    "input_state_vectors = np.concatenate(input_state_vectors,axis = 0)\n",
    "\n",
    "labels = []\n",
    "for i in range(10):\n",
    "    labels.append(i)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "labels_zero = [labels[0]]*Digit_zero.shape[0]\n",
    "labels_one  = [labels[1]]*Digit_one.shape[0]\n",
    "labels_two  = [labels[2]]*Digit_two.shape[0]\n",
    "labels_three  = [labels[3]]*Digit_three.shape[0]\n",
    "labels_four  = [labels[4]]*Digit_four.shape[0]\n",
    "labels_five  = [labels[5]]*Digit_fifth.shape[0]\n",
    "labels_six  = [labels[6]]*Digit_sixth.shape[0]\n",
    "labels_seven  = [labels[7]]*Digit_seventh.shape[0]\n",
    "labels_eigth  = [labels[8]]*Digit_eigth.shape[0]\n",
    "labels_nineth  = [labels[9]]*Digit_nineth.shape[0]\n",
    "labels_zero = np.array(labels_zero,dtype = int)\n",
    "labels_one = np.array(labels_one,dtype = int)\n",
    "labels_two = np.array(labels_two,dtype = int)\n",
    "labels_three = np.array(labels_three,dtype = int)\n",
    "labels_four = np.array(labels_four,dtype = int)\n",
    "labels_five = np.array(labels_five,dtype = int)\n",
    "labels_six = np.array(labels_six,dtype = int)\n",
    "labels_seven = np.array(labels_seven,dtype = int)\n",
    "labels_eigth = np.array(labels_eigth,dtype = int)\n",
    "labels_nineth = np.array(labels_nineth,dtype = int)\n",
    "\n",
    "labels_new_train = np.concatenate((labels_zero,labels_one))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_two))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_three))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_four))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_five))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_six))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_seven))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_eigth))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_nineth))\n",
    "\n",
    "\n",
    "target_states_combined = np.zeros((60000, 64), dtype=np.float64)\n",
    "target_states_combined[0:Digit_zero.shape[0], 0] = 1\n",
    "target_states_combined[Digit_zero.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0], 7] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]:Digit_zero.shape[0] + Digit_one.shape[0]+Digit_two.shape[0], 14] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0], 21] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0], 28] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0], 35] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0], 42] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0], 49] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0]+Digit_eigth.shape[0], 56] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0]+Digit_eigth.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0]+Digit_eigth.shape[0]+Digit_nineth.shape[0], 63] = 1\n",
    "target_states_combined = torch.tensor(target_states_combined, dtype=torch.float64)\n",
    "\n",
    "# Test Dataset \n",
    "\n",
    "# Load MNIST test dataset\n",
    "images = idx2numpy.convert_from_file(r\"D:\\Downloads Now\\archive\\\\t10k-images.idx3-ubyte\")# Download the MNIST Dataset from Kaggle and Change this directory to your Directory\n",
    "labelss = idx2numpy.convert_from_file(r\"D:\\Downloads Now\\archive\\\\t10k-labels.idx1-ubyte\")\n",
    "\n",
    "# Resize images to 32x32\n",
    "images_resized = np.array([np.array(Image.fromarray(img).resize((8, 8))) for img in images])\n",
    "\n",
    "# Normalize vectors\n",
    "test_images = images_resized \n",
    "\n",
    "input_state_test_vectors = []\n",
    "\n",
    "\n",
    "# Digit zero to Nine grey scale matrix conversion by reshaping a row array into a (N,N) size\n",
    "\n",
    "N = 8\n",
    "\n",
    "\n",
    "for i in range(len(images_resized)):\n",
    "    input_state_test_vectors.append((images_resized[i].reshape(N**2,1))/np.linalg.norm(images_resized[i]))\n",
    "\n",
    "input_state_test_vectors = np.array(input_state_test_vectors,dtype = np.float64)\n",
    "input_state_test_vectors = torch.tensor(input_state_test_vectors,dtype = torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95c0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Circuit = Quantum Unitary by 1. Conjugate Gradient Methods for finding the Unitary Matrix of Seperation ...\n",
    "# 2. Singular Value Decomposition (S.V.D)\n",
    "# 3. QR Decomposition \n",
    "# 4. Exponential Hamiltonian Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ac390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_6848\\3522300090.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_states_combined = torch.tensor(target_states_combined, dtype=torch.float64)\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_6848\\3522300090.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_state_test_vectors = torch.tensor(input_state_test_vectors,dtype = torch.float64)\n",
      "c:\\Users\\Akshay Patil\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_6848\\3522300090.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(target_states_combined, dtype=torch.float64))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/300], Loss: 6.1164e-02, Train Accuracy: 70.23167 %, Test Accuracy: 72.67000 % , Right Unitary Loss:4.9418530e-15, Left Unitary Loss:5.2268109e-15\n",
      "Epoch [1/300], Loss: 6.1065e-02, Train Accuracy: 73.07166 %, Test Accuracy: 75.01000 % , Right Unitary Loss:5.5669489e-15, Left Unitary Loss:5.4861482e-15\n",
      "Epoch [2/300], Loss: 6.1099e-02, Train Accuracy: 73.35500 %, Test Accuracy: 75.51000 % , Right Unitary Loss:5.2666767e-15, Left Unitary Loss:5.1698963e-15\n",
      "Epoch [3/300], Loss: 6.0960e-02, Train Accuracy: 73.43166 %, Test Accuracy: 75.54000 % , Right Unitary Loss:5.5722679e-15, Left Unitary Loss:5.9205204e-15\n",
      "Epoch [4/300], Loss: 6.1174e-02, Train Accuracy: 75.15833 %, Test Accuracy: 77.10001 % , Right Unitary Loss:5.5202299e-15, Left Unitary Loss:5.3403749e-15\n",
      "Epoch [5/300], Loss: 6.0972e-02, Train Accuracy: 74.25667 %, Test Accuracy: 76.22000 % , Right Unitary Loss:5.1252913e-15, Left Unitary Loss:5.2480472e-15\n",
      "Epoch [6/300], Loss: 6.0949e-02, Train Accuracy: 74.87833 %, Test Accuracy: 76.63000 % , Right Unitary Loss:5.3855003e-15, Left Unitary Loss:5.5087314e-15\n",
      "Epoch [7/300], Loss: 6.1055e-02, Train Accuracy: 75.57333 %, Test Accuracy: 77.83000 % , Right Unitary Loss:5.3732237e-15, Left Unitary Loss:5.5541434e-15\n",
      "Epoch [8/300], Loss: 6.0819e-02, Train Accuracy: 72.92500 %, Test Accuracy: 75.43000 % , Right Unitary Loss:5.4341976e-15, Left Unitary Loss:5.3543719e-15\n",
      "Epoch [9/300], Loss: 6.0882e-02, Train Accuracy: 74.42667 %, Test Accuracy: 76.73000 % , Right Unitary Loss:5.4590182e-15, Left Unitary Loss:5.3275852e-15\n",
      "Epoch [10/300], Loss: 6.0885e-02, Train Accuracy: 75.15666 %, Test Accuracy: 77.08000 % , Right Unitary Loss:5.3159824e-15, Left Unitary Loss:5.3150782e-15\n",
      "Epoch [11/300], Loss: 6.0786e-02, Train Accuracy: 73.33500 %, Test Accuracy: 75.58000 % , Right Unitary Loss:5.4874628e-15, Left Unitary Loss:5.5377447e-15\n",
      "Epoch [12/300], Loss: 6.0882e-02, Train Accuracy: 74.69000 %, Test Accuracy: 76.43000 % , Right Unitary Loss:5.7742560e-15, Left Unitary Loss:5.7518447e-15\n",
      "Epoch [13/300], Loss: 6.1021e-02, Train Accuracy: 74.86167 %, Test Accuracy: 77.02000 % , Right Unitary Loss:5.7594729e-15, Left Unitary Loss:5.5855677e-15\n",
      "Epoch [14/300], Loss: 6.0958e-02, Train Accuracy: 75.02500 %, Test Accuracy: 77.30000 % , Right Unitary Loss:5.5312887e-15, Left Unitary Loss:5.8315200e-15\n",
      "Epoch [15/300], Loss: 6.0843e-02, Train Accuracy: 73.02167 %, Test Accuracy: 75.16000 % , Right Unitary Loss:5.4531277e-15, Left Unitary Loss:5.6831749e-15\n",
      "Epoch [16/300], Loss: 6.0925e-02, Train Accuracy: 74.21500 %, Test Accuracy: 76.52000 % , Right Unitary Loss:5.4178477e-15, Left Unitary Loss:5.3302528e-15\n",
      "Epoch [17/300], Loss: 6.0951e-02, Train Accuracy: 74.95167 %, Test Accuracy: 77.37000 % , Right Unitary Loss:5.2347863e-15, Left Unitary Loss:5.2075039e-15\n",
      "Epoch [18/300], Loss: 6.0798e-02, Train Accuracy: 74.64333 %, Test Accuracy: 76.80000 % , Right Unitary Loss:5.4562415e-15, Left Unitary Loss:5.2892445e-15\n",
      "Epoch [19/300], Loss: 6.0909e-02, Train Accuracy: 74.52666 %, Test Accuracy: 76.70000 % , Right Unitary Loss:5.4038430e-15, Left Unitary Loss:5.4193498e-15\n",
      "Epoch [20/300], Loss: 6.0857e-02, Train Accuracy: 74.25167 %, Test Accuracy: 76.50000 % , Right Unitary Loss:5.6151338e-15, Left Unitary Loss:5.5905226e-15\n",
      "Epoch [21/300], Loss: 6.0957e-02, Train Accuracy: 74.41000 %, Test Accuracy: 76.55000 % , Right Unitary Loss:5.3706235e-15, Left Unitary Loss:5.4005272e-15\n",
      "Epoch [22/300], Loss: 6.0814e-02, Train Accuracy: 74.48000 %, Test Accuracy: 76.68000 % , Right Unitary Loss:5.1893333e-15, Left Unitary Loss:5.0516857e-15\n",
      "Epoch [23/300], Loss: 6.0933e-02, Train Accuracy: 74.93833 %, Test Accuracy: 77.13000 % , Right Unitary Loss:5.9107047e-15, Left Unitary Loss:5.9761685e-15\n",
      "Epoch [24/300], Loss: 6.0870e-02, Train Accuracy: 74.43666 %, Test Accuracy: 76.73000 % , Right Unitary Loss:5.6922986e-15, Left Unitary Loss:5.6384439e-15\n",
      "Epoch [25/300], Loss: 6.0947e-02, Train Accuracy: 74.23167 %, Test Accuracy: 76.63000 % , Right Unitary Loss:5.7927613e-15, Left Unitary Loss:5.7474118e-15\n",
      "Epoch [26/300], Loss: 6.0930e-02, Train Accuracy: 74.93666 %, Test Accuracy: 77.46001 % , Right Unitary Loss:5.3482929e-15, Left Unitary Loss:5.2741789e-15\n",
      "Epoch [27/300], Loss: 6.0937e-02, Train Accuracy: 74.96167 %, Test Accuracy: 77.07000 % , Right Unitary Loss:5.2027802e-15, Left Unitary Loss:5.1721538e-15\n",
      "Epoch [28/300], Loss: 6.1044e-02, Train Accuracy: 74.84834 %, Test Accuracy: 76.89000 % , Right Unitary Loss:5.5736740e-15, Left Unitary Loss:5.4928037e-15\n",
      "Epoch [29/300], Loss: 6.0949e-02, Train Accuracy: 74.87500 %, Test Accuracy: 76.91000 % , Right Unitary Loss:5.5605366e-15, Left Unitary Loss:5.6184436e-15\n",
      "Epoch [30/300], Loss: 6.0824e-02, Train Accuracy: 74.63334 %, Test Accuracy: 76.86000 % , Right Unitary Loss:5.6651176e-15, Left Unitary Loss:5.6574156e-15\n",
      "Epoch [31/300], Loss: 6.0987e-02, Train Accuracy: 74.88167 %, Test Accuracy: 77.00000 % , Right Unitary Loss:5.5926845e-15, Left Unitary Loss:5.5967448e-15\n",
      "Epoch [32/300], Loss: 6.0944e-02, Train Accuracy: 74.71833 %, Test Accuracy: 76.83000 % , Right Unitary Loss:5.2844351e-15, Left Unitary Loss:5.2969746e-15\n"
     ]
    }
   ],
   "source": [
    "# 1. Conjugate Gradient Methods for finding the Unitary Matrix of Seperation ...\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import scipy.linalg as la\n",
    "import pennylane as qml\n",
    "\n",
    "input_state_vectors = np.array(input_state_vectors, dtype=np.float64)\n",
    "input_vectors = torch.tensor(input_state_vectors, dtype=torch.float64)\n",
    "\n",
    "\n",
    "target_states_combined = np.zeros((60000, 64), dtype=np.float64)\n",
    "target_states_combined[0:Digit_zero.shape[0], 0] = 1\n",
    "target_states_combined[Digit_zero.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0], 7] =  1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]:Digit_zero.shape[0] + Digit_one.shape[0]+Digit_two.shape[0], 14] =  1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0], 21] =  1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0], 28] =  1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0], 35] =  1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0], 42] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0], 49] =  1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0]+Digit_eigth.shape[0], 56] = 1\n",
    "target_states_combined[Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0]+Digit_eigth.shape[0]:Digit_zero.shape[0]+Digit_one.shape[0]+Digit_two.shape[0]+Digit_three.shape[0]+Digit_four.shape[0]+Digit_fifth.shape[0]+Digit_sixth.shape[0]+Digit_seventh.shape[0]+Digit_eigth.shape[0]+Digit_nineth.shape[0], 63] =  1 \n",
    "target_states_combined = torch.tensor(target_states_combined, dtype=torch.float64)\n",
    "\n",
    "\n",
    "target_states_combined = torch.tensor(target_states_combined, dtype=torch.float64)\n",
    "labels_train = torch.tensor(labels_new_train , dtype = torch.long)\n",
    "labels_test = torch.tensor(labelss , dtype = torch.long)\n",
    "input_state_test_vectors = torch.tensor(input_state_test_vectors,dtype = torch.float64)\n",
    "\n",
    "\n",
    "# Define the Machine Learning By Conjugate Gradients Model\n",
    "class UnitaryMatrixModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.fc = nn.Linear(32, 32, bias=False,dtype = torch.float64) # for Unitary Block Encoding Method\n",
    "        self.fc = nn.Linear(64, 64, bias=False,dtype = torch.float64)\n",
    "\n",
    "    # def make_unitary(self, matrix):\n",
    "    #     Q, R = torch.linalg.qr(matrix)\n",
    "    #     return Q\n",
    "    def make_unitary(self,matrix):\n",
    "        q, r = torch.linalg.qr(matrix)\n",
    "        ph   = torch.sign(torch.diag(r))\n",
    "        return q * ph\n",
    "    # def make_unitary(self,matrix):\n",
    "    #     # matrix = matrix.detach().numpy()\n",
    "    #     u, s,v = torch.linalg.svd(matrix)\n",
    "    #     unitary = u@v\n",
    "    #     return unitary\n",
    "    # def make_unitary(self,matrix):\n",
    "    #     matrix = (matrix + matrix.conj().T)/2\n",
    "    #     s = torch.matrix_exp((-1j)*matrix)\n",
    "    #     return s\n",
    "    # def make_non_unitary(self,matrix): # Nonunitary Matrices\n",
    "    #     return matrix\n",
    "    # def make_unitary(self,matrix):\n",
    "    #     # Requires 32,32 Linear Layer Matrix , which can be block encoded to create 64 x 64 matrix...\n",
    "    #     num_qubits = 6\n",
    "    #     dev = qml.device('default.qubit', wires = num_qubits)\n",
    "    #     op = qml.BlockEncode(matrix, wires = range(num_qubits))\n",
    "    #     unitary = qml.matrix(op)\n",
    "    #     return unitary\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # U = self.make_unitary(self.fc.weight)\n",
    "        U = self.make_unitary(self.fc.weight)\n",
    "        # U = self.fc.weight # for orthonormality constraint\n",
    "        x = U@x.reshape(-1,64,1)\n",
    "        x = x.reshape(-1,64)\n",
    "        return x,U\n",
    "\n",
    "# Finding the Accuracy of the Classification from the Quantum State Vector prior to Measurement , but also can be extended \n",
    "# to after measurement , from the resultant state that occurs most number of times and comparing the index position of that \n",
    "# with respect to the set target state vectors probability amplitudes , ex - if most number of times the state after measurement \n",
    "# is |64> i.e. |Binary form of 64> , then the class of the input vector predicted by this basic quantum model is class 1 ...\n",
    "\n",
    "# Output Vectors before Quantum Measurement by ML Method\n",
    "\n",
    "def evaluate_model(model , labels_train , labels_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_train,U = model(input_state_vectors)\n",
    "        output_test,U = model(input_state_test_vectors)\n",
    "        pred_classes_train = torch.argmax(output_train,dim=1)//7 # as 7 is the class element spacing of the extended one hot label encoding of target vectors\n",
    "        pred_classes_test = torch.argmax(output_test,dim=1)//7\n",
    "        Train_accuracy = torch.sum(pred_classes_train == labels_train)/60000\n",
    "        Test_accuracy = torch.sum(pred_classes_test == labels_test)/10000\n",
    "        return Train_accuracy*100 , Test_accuracy*100\n",
    "\n",
    "# Since this is Quantum Inspired Approach No Need of Quantum Measurement but one can try Quantum Measurement by Indexing Without Using Qiskit but directly using \n",
    "# numpy or pytorch or Tensorflow or Scipy using Indexing Approach.\n",
    "\n",
    "\n",
    "# Custom loss function combining RMSE and orthonormality\n",
    "def combined_loss(output, target,M):\n",
    "    rmse_loss_between_probabilities = nn.CrossEntropyLoss()(output, target) # MSE between the Output and the Target Probabilities Reduction\n",
    "    # This can be changed with other Loss functions as \n",
    "    # well as Locally Weighted Linear Regression to fit the Nonlinear Dataset can be used (Degree of Dataset not equal to 1.)\n",
    "    # Orthonormality loss\n",
    "    gram_matrix_0 = torch.mm(M.conj().T, M)  # Compute Gram matrix\n",
    "    identity_0 = torch.eye(M.shape[1]).to(M.device)  # Identity matrix for columns\n",
    "    ortho_loss_0 = torch.norm(gram_matrix_0 - identity_0)  # Frobenius norm of the difference\n",
    "    gram_matrix_1 = torch.mm(M, M.conj().T)  # Compute Gram matrix\n",
    "    identity_1 = torch.eye(M.shape[1]).to(M.device)  # Identity matrix for columns\n",
    "    ortho_loss_1 = torch.norm(gram_matrix_1 - identity_1)  # Frobenius norm of the difference\n",
    "\n",
    "    return rmse_loss_between_probabilities,ortho_loss_0 ,ortho_loss_1  #rmse_loss_between_probabilities + ortho_loss_0 + ortho_loss_1 , ortho_loss_0 ,ortho_loss_1 # One can also give the Orthonormality constraints in the optimization \n",
    "#equation to create the unitary (orthonormal) matrix ...\n",
    "\n",
    "\n",
    "# Initialize model, optimizer, and data\n",
    "model = UnitaryMatrixModel()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-3) # Experiment with learing rate for different methods and also different Optimizer\n",
    "\n",
    "scheduler   = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, factor=0.5 , verbose = True\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(input_state_vectors, dtype=torch.float64),\n",
    "                             torch.tensor(target_states_combined, dtype=torch.float64))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True, num_workers=0)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for a,b in train_loader:\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        output,U = model(a)     \n",
    "        loss , rightortho , leftortho = combined_loss(output, b, U)  # Calculate combined loss\n",
    "        loss.backward()         # Backpropagation\n",
    "        optimizer.step()        # Update weights\n",
    "    scheduler.step(loss/len(train_loader))\n",
    "    train_accuracy,test_accuracy = evaluate_model(model,labels_train , labels_test)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item()/len(train_loader):.4e}, Train Accuracy: {train_accuracy:.5f} %, Test Accuracy: {test_accuracy:.5f} % , Right Unitary Loss:{rightortho:.7e}, Left Unitary Loss:{leftortho:.7e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a82d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r\"C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_\\UnitaryFinalClassifierOrthonormalityConstraint.npy\",U.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28055821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quantum Amplitude Encoding\n",
    "\n",
    "# Base Single Unitary Quantum Circuits for Classification in Amplitude Encoding...\n",
    "\n",
    "# Input Image Dimension of Classical Data - 8 x 8 -> 64 dimensional to save the compute power...\n",
    "\n",
    "\n",
    "# Results - Train Accuracy | Test Accuracy | Average Circuit Depth of Amplitude Encoded Input Vectors and Quantum Circuit by Quantum Shanon Decomposition Per Input Data Point by SVD for Input Mapping (d_svd)\n",
    "#  | Average Total Gates Per Input Data Point by SVD for Input Mapping (g_svd) | d_gradients | g_gradients | d_qr | g_qr | d_qr_diag | g_qr_diag | d_unitary_encoding | g_unitary_encoding | d_exp | g_exp | \n",
    "\n",
    "# Different Initialization , different learning rates and small different relaxation in unitary can give subtle increase in convergence accuracy but not above the non-unitary threshold...\n",
    "# A.Real Unitary Real INput Real Target By Conjugate Gradients Methods - #1. Make Unitary by QR - 58.62%|60.1600%|3500|3840| - Rough Training - Accuracy irrespective of Loss increase and decrease\n",
    "                                            #1. Make Unitary by QR Stabilized - 74 %|77 %|3500|3840|3500|3840||||||| - Stabilized\n",
    "                                           #2. Make Unitary by SVD - 74%| 77%|3500|3840|\n",
    "                                           #3. Make Unitary by Orthonormal Loss Function - 71.75%| 73.97%|3500|3840|\n",
    "                                           #4. Make Unitary by Exponential of Hermitian Tranable Hamiltonian of the Quantum Circuit of Classification - %| %| | | - Works only for complex Unitaries which requires complex input and output , will upload it later along with all other possible encodings and architechtures!!!\n",
    "                                           #5. Make Unitary by Unitary Encoding- 10.4333%| 9.86%|3500|3840|  - Random Accuracy - Here the target vectors can be changed to just top 32 elements of the 64 x 1 vectors as only those are trainable , and other ways can also be thought out to increase the accuracy\n",
    "                                           #6. Make Non-Unitary - 87.30%| 88.73%|-|-| (without Bias conventional Linear Regression)\n",
    "                                           # The Average Depth Remain Same for all methods as the decomposition Method of Unitary of Mapping and Unitary of\n",
    "                                           # of classification always use SVD ,still there should be difference as the matrix method change so elements changes\n",
    "                                           # of classification, so one can use other methods there for decomposition (one of the above to see if circuit depth minimizes or remain same)\n",
    "                                           # if they remain same , then one can say experimentally that only the size of the unitary matrix and precision of the unitary decides the optimum depth and number of gates\n",
    "                                           # irrespective of elements which is governed by the underlying method used for finding unitary - \"It remains the same\" ...\n",
    "# B.Real Unitary Real INput Real Target Unitary By Direct SVD - %| %| | |\n",
    "# C.Real Unitary Real INput Real Target Unitary By QR Decomposition - %| %| | |\n",
    "# D.Real Unitary Real INput Real Target Unitary By Exponential of Hermitian Trainable Hamiltonian of the Quantum Circuit of Classification - %| %| | |\n",
    "# E.Real Unitary Real INput Real Target Unitary By Unitary Encoding - %| %| | |\n",
    "# F.Real Unitary Real INput Real Target Unitary By Non Unitary (Open Quantum System) - %| %| | |\n",
    "\n",
    "# Note - We are not putting in restriction of connectivity of CNOT's available on Present Quantum Computers \n",
    "# If connectivity restirictions are imposed the Number of Gates can increase to approximate the same unitary  , also the Fildelity of the \n",
    "# circuit to approximate the given unitaries can also be low , but here with QS Decompostion We have perfect Fildelity upto 10e-15\n",
    "\n",
    "# Note - Use of Standard Scaler and Minimax Scaler as Preprocessing Step can increase the Accuracy , also to save the compute we have taken only 8,8\n",
    "# images  , 32,32 can achieve a highest upto 80 Percent on test , since we this is base quantum inspired ML algorithm only quantum linear unitary\n",
    "# constriant linear classification is possible without bias...We will add on more advanced methods on this later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d66c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 000000: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000001: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000002: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000003: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000004: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000005: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000006: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000007: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000008: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000009: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000010: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000011: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000012: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000013: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000014: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000015: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000016: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000017: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000018: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000019: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000020: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000021: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000022: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000023: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000024: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000025: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000026: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000027: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000028: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000029: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000030: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000031: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000032: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000033: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000034: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000035: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000036: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000037: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000038: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000039: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000040: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000041: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000042: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000043: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000044: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000045: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000046: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000047: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000048: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000049: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000050: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000051: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000052: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000053: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000054: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000055: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000056: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000057: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000058: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000059: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000060: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000061: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000062: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000063: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000064: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000065: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000066: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000067: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000068: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000069: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000070: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000071: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000072: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000073: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000074: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000075: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000076: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000077: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000078: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000079: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000010: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000011: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000012: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000013: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000014: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000015: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000016: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000017: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000018: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000019: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000020: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000021: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000022: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000023: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000024: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000025: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000026: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000027: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000028: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000029: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000030: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000031: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000032: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000033: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000034: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000035: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000036: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000037: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000038: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000039: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000040: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000041: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000042: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000043: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000044: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000045: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000046: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000047: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000048: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000049: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000050: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000051: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000052: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000053: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000054: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000055: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000056: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000057: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000058: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000059: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000060: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000061: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000062: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000063: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000064: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000065: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000066: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000067: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000068: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000069: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000070: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000071: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000072: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000073: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000074: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000075: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000076: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000077: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000078: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000079: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000020: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000021: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000022: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000023: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000024: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000025: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000026: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000027: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000028: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000029: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000030: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000031: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000032: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000033: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000034: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000035: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000036: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000037: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000038: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000039: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000040: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000041: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000042: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000043: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000044: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000045: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000046: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000047: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000048: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000049: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000050: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000051: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000052: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000053: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000054: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000055: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000056: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000057: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000058: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000059: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000060: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000061: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000062: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000063: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000064: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000065: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000066: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000067: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000068: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000069: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000070: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000071: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000072: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000073: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000074: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000075: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000076: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000077: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000078: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000079: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000110: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000111: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000112: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000113: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000114: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000115: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000116: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000117: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000118: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000119: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000030: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000031: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000032: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000033: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000034: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000035: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000036: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000037: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000038: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000039: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000040: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000041: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000042: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000043: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000044: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000045: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000046: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000047: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000048: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000049: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000050: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000051: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000052: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000053: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000054: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000055: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000056: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000057: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000058: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000059: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000060: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000061: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000062: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000063: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000064: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000065: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000066: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000067: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000068: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000069: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000070: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000071: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000072: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000073: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000074: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000075: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000076: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000077: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000078: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000079: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000110: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000111: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000112: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000113: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000114: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000115: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000116: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000117: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000118: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000119: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000120: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000121: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000122: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000123: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000124: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000125: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000126: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000127: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000128: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000129: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000040: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000041: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000042: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000043: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000044: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000045: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000046: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000047: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000048: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000049: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000050: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000051: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000052: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000053: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000054: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000055: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000056: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000057: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000058: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000059: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000060: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000061: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000062: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000063: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000064: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000065: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000066: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000067: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000068: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000069: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000070: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000071: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000072: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000073: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000074: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000075: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000076: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000077: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000078: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000079: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000110: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000111: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000112: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000113: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000114: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000115: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000116: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000117: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000118: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000119: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000120: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000121: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000122: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000123: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000124: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000125: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000126: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000127: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000128: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000129: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000130: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000131: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000132: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000133: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000134: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000135: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000136: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000137: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000138: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000139: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000050: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000051: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000052: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000053: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000054: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000055: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000056: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000057: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000058: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000059: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000060: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000061: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000062: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000063: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000064: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000065: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000066: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000067: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000068: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000069: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000070: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000071: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000072: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000073: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000074: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000075: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000076: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000077: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000078: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000079: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000110: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000111: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000112: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000113: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000114: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000115: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000116: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000117: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000118: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000119: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000120: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000121: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000122: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000123: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000124: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000125: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000126: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000127: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000128: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000129: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000130: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000131: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000132: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000133: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000134: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000135: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000136: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000137: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000138: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000139: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000140: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000141: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000142: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000143: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000144: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000145: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000146: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000147: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000148: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000149: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000060: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000061: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000062: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000063: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000064: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000065: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000066: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000067: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000068: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000069: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000070: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000071: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000072: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000073: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000074: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000075: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000076: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000077: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000078: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000079: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000110: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000111: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000112: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000113: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000114: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000115: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000116: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000117: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000118: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000119: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000120: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000121: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000122: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000123: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000124: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000125: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000126: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000127: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000128: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000129: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000130: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000131: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000132: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000133: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000134: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000135: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000136: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000137: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000138: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000139: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000140: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000141: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000142: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000143: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000144: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000145: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000146: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000147: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000148: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000149: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000150: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000151: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000152: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000153: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000154: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000155: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000156: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000157: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000158: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000159: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000070: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000071: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000072: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000073: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000074: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000075: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000076: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000077: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000078: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000079: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000110: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000111: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000112: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000113: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000114: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000115: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000116: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000117: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000118: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000119: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000120: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000121: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000122: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000123: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000124: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000125: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000126: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000127: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000128: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000129: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000130: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000131: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000132: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000133: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000134: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000135: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000136: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000137: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000138: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000139: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000140: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000141: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000142: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000143: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000144: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000145: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000146: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000147: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000148: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000149: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000150: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000151: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000152: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000153: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000154: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000155: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000156: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000157: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000158: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000159: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000160: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000161: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000162: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000163: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000164: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000165: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000166: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000167: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000168: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000169: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000080: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000081: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000082: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000083: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000084: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000085: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000086: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000087: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000088: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000089: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000110: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000111: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000112: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000113: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000114: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000115: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000116: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000117: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000118: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000119: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000120: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000121: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000122: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000123: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000124: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000125: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000126: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000127: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000128: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000129: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000130: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000131: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000132: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000133: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000134: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000135: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000136: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000137: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000138: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000139: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000140: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000141: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000142: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000143: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000144: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000145: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000146: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000147: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000148: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000149: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000150: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000151: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000152: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000153: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000154: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000155: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000156: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000157: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000158: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000159: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000160: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000161: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000162: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000163: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000164: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000165: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000166: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000167: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000168: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000169: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000170: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000171: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000172: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000173: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000174: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000175: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000176: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000177: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000178: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000179: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "Pair 000090: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005187+0.00000000000000000000j\n",
      "Pair 000091: Unitarity error = 8.30e-15, Mapping RMSE = 0.00000000000000007003+0.00000000000000000000j\n",
      "Pair 000092: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000005032+0.00000000000000000000j\n",
      "Pair 000093: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000005310+0.00000000000000000000j\n",
      "Pair 000094: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000005500+0.00000000000000000000j\n",
      "Pair 000095: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000005271+0.00000000000000000000j\n",
      "Pair 000096: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000005872+0.00000000000000000000j\n",
      "Pair 000097: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005438+0.00000000000000000000j\n",
      "Pair 000098: Unitarity error = 8.22e-15, Mapping RMSE = 0.00000000000000006564+0.00000000000000000000j\n",
      "Pair 000099: Unitarity error = 8.63e-15, Mapping RMSE = 0.00000000000000007433+0.00000000000000000000j\n",
      "Pair 000100: Unitarity error = 7.28e-15, Mapping RMSE = 0.00000000000000005351+0.00000000000000000000j\n",
      "Pair 000101: Unitarity error = 9.08e-15, Mapping RMSE = 0.00000000000000007730+0.00000000000000000000j\n",
      "Pair 000102: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005980+0.00000000000000000000j\n",
      "Pair 000103: Unitarity error = 8.66e-15, Mapping RMSE = 0.00000000000000005382+0.00000000000000000000j\n",
      "Pair 000104: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000005185+0.00000000000000000000j\n",
      "Pair 000105: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000005791+0.00000000000000000000j\n",
      "Pair 000106: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000009315+0.00000000000000000000j\n",
      "Pair 000107: Unitarity error = 8.16e-15, Mapping RMSE = 0.00000000000000004873+0.00000000000000000000j\n",
      "Pair 000108: Unitarity error = 7.76e-15, Mapping RMSE = 0.00000000000000005818+0.00000000000000000000j\n",
      "Pair 000109: Unitarity error = 8.41e-15, Mapping RMSE = 0.00000000000000006771+0.00000000000000000000j\n",
      "Pair 000110: Unitarity error = 8.55e-15, Mapping RMSE = 0.00000000000000006459+0.00000000000000000000j\n",
      "Pair 000111: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000007009+0.00000000000000000000j\n",
      "Pair 000112: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000004835+0.00000000000000000000j\n",
      "Pair 000113: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000004804+0.00000000000000000000j\n",
      "Pair 000114: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000006713+0.00000000000000000000j\n",
      "Pair 000115: Unitarity error = 8.33e-15, Mapping RMSE = 0.00000000000000007340+0.00000000000000000000j\n",
      "Pair 000116: Unitarity error = 7.38e-15, Mapping RMSE = 0.00000000000000008406+0.00000000000000000000j\n",
      "Pair 000117: Unitarity error = 9.62e-15, Mapping RMSE = 0.00000000000000006719+0.00000000000000000000j\n",
      "Pair 000118: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000007122+0.00000000000000000000j\n",
      "Pair 000119: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000007910+0.00000000000000000000j\n",
      "Pair 000120: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000007403+0.00000000000000000000j\n",
      "Pair 000121: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000007292+0.00000000000000000000j\n",
      "Pair 000122: Unitarity error = 8.88e-15, Mapping RMSE = 0.00000000000000005824+0.00000000000000000000j\n",
      "Pair 000123: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000006528+0.00000000000000000000j\n",
      "Pair 000124: Unitarity error = 8.96e-15, Mapping RMSE = 0.00000000000000005721+0.00000000000000000000j\n",
      "Pair 000125: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000004875+0.00000000000000000000j\n",
      "Pair 000126: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000004694+0.00000000000000000000j\n",
      "Pair 000127: Unitarity error = 7.90e-15, Mapping RMSE = 0.00000000000000004528+0.00000000000000000000j\n",
      "Pair 000128: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000006512+0.00000000000000000000j\n",
      "Pair 000129: Unitarity error = 8.14e-15, Mapping RMSE = 0.00000000000000005886+0.00000000000000000000j\n",
      "Pair 000130: Unitarity error = 8.79e-15, Mapping RMSE = 0.00000000000000005539+0.00000000000000000000j\n",
      "Pair 000131: Unitarity error = 8.89e-15, Mapping RMSE = 0.00000000000000007666+0.00000000000000000000j\n",
      "Pair 000132: Unitarity error = 8.59e-15, Mapping RMSE = 0.00000000000000004459+0.00000000000000000000j\n",
      "Pair 000133: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000006007+0.00000000000000000000j\n",
      "Pair 000134: Unitarity error = 8.53e-15, Mapping RMSE = 0.00000000000000004887+0.00000000000000000000j\n",
      "Pair 000135: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007069+0.00000000000000000000j\n",
      "Pair 000136: Unitarity error = 9.10e-15, Mapping RMSE = 0.00000000000000005586+0.00000000000000000000j\n",
      "Pair 000137: Unitarity error = 8.87e-15, Mapping RMSE = 0.00000000000000005042+0.00000000000000000000j\n",
      "Pair 000138: Unitarity error = 9.12e-15, Mapping RMSE = 0.00000000000000006045+0.00000000000000000000j\n",
      "Pair 000139: Unitarity error = 8.46e-15, Mapping RMSE = 0.00000000000000005301+0.00000000000000000000j\n",
      "Pair 000140: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005860+0.00000000000000000000j\n",
      "Pair 000141: Unitarity error = 8.31e-15, Mapping RMSE = 0.00000000000000004359+0.00000000000000000000j\n",
      "Pair 000142: Unitarity error = 8.38e-15, Mapping RMSE = 0.00000000000000004848+0.00000000000000000000j\n",
      "Pair 000143: Unitarity error = 8.60e-15, Mapping RMSE = 0.00000000000000005440+0.00000000000000000000j\n",
      "Pair 000144: Unitarity error = 8.36e-15, Mapping RMSE = 0.00000000000000008661+0.00000000000000000000j\n",
      "Pair 000145: Unitarity error = 7.63e-15, Mapping RMSE = 0.00000000000000004703+0.00000000000000000000j\n",
      "Pair 000146: Unitarity error = 8.07e-15, Mapping RMSE = 0.00000000000000004589+0.00000000000000000000j\n",
      "Pair 000147: Unitarity error = 8.62e-15, Mapping RMSE = 0.00000000000000005653+0.00000000000000000000j\n",
      "Pair 000148: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000007644+0.00000000000000000000j\n",
      "Pair 000149: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000005731+0.00000000000000000000j\n",
      "Pair 000150: Unitarity error = 7.98e-15, Mapping RMSE = 0.00000000000000006020+0.00000000000000000000j\n",
      "Pair 000151: Unitarity error = 8.00e-15, Mapping RMSE = 0.00000000000000008446+0.00000000000000000000j\n",
      "Pair 000152: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006000+0.00000000000000000000j\n",
      "Pair 000153: Unitarity error = 8.35e-15, Mapping RMSE = 0.00000000000000005620+0.00000000000000000000j\n",
      "Pair 000154: Unitarity error = 8.09e-15, Mapping RMSE = 0.00000000000000006650+0.00000000000000000000j\n",
      "Pair 000155: Unitarity error = 8.69e-15, Mapping RMSE = 0.00000000000000005277+0.00000000000000000000j\n",
      "Pair 000156: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000005709+0.00000000000000000000j\n",
      "Pair 000157: Unitarity error = 8.25e-15, Mapping RMSE = 0.00000000000000004811+0.00000000000000000000j\n",
      "Pair 000158: Unitarity error = 8.29e-15, Mapping RMSE = 0.00000000000000006507+0.00000000000000000000j\n",
      "Pair 000159: Unitarity error = 8.39e-15, Mapping RMSE = 0.00000000000000005256+0.00000000000000000000j\n",
      "Pair 000160: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000007104+0.00000000000000000000j\n",
      "Pair 000161: Unitarity error = 8.02e-15, Mapping RMSE = 0.00000000000000006119+0.00000000000000000000j\n",
      "Pair 000162: Unitarity error = 8.27e-15, Mapping RMSE = 0.00000000000000008452+0.00000000000000000000j\n",
      "Pair 000163: Unitarity error = 7.84e-15, Mapping RMSE = 0.00000000000000005053+0.00000000000000000000j\n",
      "Pair 000164: Unitarity error = 8.34e-15, Mapping RMSE = 0.00000000000000006404+0.00000000000000000000j\n",
      "Pair 000165: Unitarity error = 7.93e-15, Mapping RMSE = 0.00000000000000005074+0.00000000000000000000j\n",
      "Pair 000166: Unitarity error = 8.85e-15, Mapping RMSE = 0.00000000000000005890+0.00000000000000000000j\n",
      "Pair 000167: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000005869+0.00000000000000000000j\n",
      "Pair 000168: Unitarity error = 8.70e-15, Mapping RMSE = 0.00000000000000004288+0.00000000000000000000j\n",
      "Pair 000169: Unitarity error = 8.49e-15, Mapping RMSE = 0.00000000000000006505+0.00000000000000000000j\n",
      "Pair 000170: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000008439+0.00000000000000000000j\n",
      "Pair 000171: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000006112+0.00000000000000000000j\n",
      "Pair 000172: Unitarity error = 8.54e-15, Mapping RMSE = 0.00000000000000005384+0.00000000000000000000j\n",
      "Pair 000173: Unitarity error = 8.37e-15, Mapping RMSE = 0.00000000000000005734+0.00000000000000000000j\n",
      "Pair 000174: Unitarity error = 8.57e-15, Mapping RMSE = 0.00000000000000006663+0.00000000000000000000j\n",
      "Pair 000175: Unitarity error = 9.44e-15, Mapping RMSE = 0.00000000000000005314+0.00000000000000000000j\n",
      "Pair 000176: Unitarity error = 7.67e-15, Mapping RMSE = 0.00000000000000005281+0.00000000000000000000j\n",
      "Pair 000177: Unitarity error = 8.45e-15, Mapping RMSE = 0.00000000000000007499+0.00000000000000000000j\n",
      "Pair 000178: Unitarity error = 8.67e-15, Mapping RMSE = 0.00000000000000007080+0.00000000000000000000j\n",
      "Pair 000179: Unitarity error = 8.65e-15, Mapping RMSE = 0.00000000000000006688+0.00000000000000000000j\n",
      "Pair 000180: Unitarity error = 8.28e-15, Mapping RMSE = 0.00000000000000005325+0.00000000000000000000j\n",
      "Pair 000181: Unitarity error = 7.64e-15, Mapping RMSE = 0.00000000000000005954+0.00000000000000000000j\n",
      "Pair 000182: Unitarity error = 8.20e-15, Mapping RMSE = 0.00000000000000005043+0.00000000000000000000j\n",
      "Pair 000183: Unitarity error = 8.42e-15, Mapping RMSE = 0.00000000000000004972+0.00000000000000000000j\n",
      "Pair 000184: Unitarity error = 8.10e-15, Mapping RMSE = 0.00000000000000005650+0.00000000000000000000j\n",
      "Pair 000185: Unitarity error = 8.61e-15, Mapping RMSE = 0.00000000000000004604+0.00000000000000000000j\n",
      "Pair 000186: Unitarity error = 8.05e-15, Mapping RMSE = 0.00000000000000006855+0.00000000000000000000j\n",
      "Pair 000187: Unitarity error = 7.87e-15, Mapping RMSE = 0.00000000000000005589+0.00000000000000000000j\n",
      "Pair 000188: Unitarity error = 7.94e-15, Mapping RMSE = 0.00000000000000005172+0.00000000000000000000j\n",
      "Pair 000189: Unitarity error = 8.71e-15, Mapping RMSE = 0.00000000000000007150+0.00000000000000000000j\n",
      "All unitary matrices saved batchwise in: C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_QR\n"
     ]
    }
   ],
   "source": [
    "# Input Data Mapping on a Quantum Computer...\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import svd,qr\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "USER_OUT_DIR = Path(r\"C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_QR\")  # Output directory\n",
    "BATCH_SIZE = 10 \n",
    "\n",
    "os.makedirs(USER_OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def input_mapping_unitary(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    #find unitary U such that x @ U  y # One can experiment here by using various Methods of Unitary mapping used above\n",
    "    #Conjugate Methods can also be used here to find the unitary of mapping from input qubit ground state to inputstatevectors \n",
    "    C = x.T @ y\n",
    "    U, _, Vt = svd(C)\n",
    "    return U @ Vt\n",
    "\n",
    "# def input_mapping_unitary(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "#     C = x.T @ y\n",
    "#     Q, R = qr(C)\n",
    "#     return Q # Does not map and causes RMSE error to be high and hence either SVD or Gradient method is more suited for unitary of mapping finding per ground-input_quantum_encoded_state\n",
    "# def input_mapping_unitary(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "#     C = x.T @ y\n",
    "#     q, r = qr(C)\n",
    "#     ph   = np.sign(np.diag(r))\n",
    "#     return q * ph # Does not map and causes RMSE error to be high and hence either SVD or Gradient method is more suited for unitary of mapping finding per ground-input_quantum_encoded_state\n",
    "\n",
    "\n",
    "# def input_mapping_unitary(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "#         # Requires 32,32 Linear Layer Matrix , which can be block encoded to create 64 x 64 matrix...\n",
    "#     matrix = x.T @ y\n",
    "#     num_qubits = 6\n",
    "#     dev = qml.device('default.qubit', wires = num_qubits)\n",
    "#     op = qml.BlockEncode(matrix, wires = range(num_qubits))\n",
    "#     unitary = qml.matrix(op)\n",
    "#     return unitary # Cannot be used due to dimension mismatch as unitary encoding by definition requires 1 extra qubit and the matrix is already 64x64\n",
    "\n",
    "\n",
    "def fro_error(U: np.ndarray) -> float:\n",
    "    # ||UU^T - I||_F\n",
    "    return np.linalg.norm(U @ U.conj().T - np.eye(U.shape[0]), \"fro\")\n",
    "\n",
    "def mapping_loss(x: np.ndarray, y: np.ndarray, U: np.ndarray) -> float:\n",
    "    # RMSE between y and x @ U\n",
    "    pred = x @ U\n",
    "    return np.sqrt(((y - pred) ** 2).mean())\n",
    "\n",
    "def save_and_delete(arr: np.ndarray, path: Path):\n",
    "    np.save(path, arr)\n",
    "    del arr\n",
    "\n",
    "Inputgroundqubitstate = np.zeros((60000,64),dtype = np.complex128)\n",
    "Inputgroundqubitstate[:,0] = 1  \n",
    "train_inputs = np.array(input_state_vectors,dtype = np.complex128)\n",
    "n_samples = 100\n",
    "\n",
    "\n",
    "for start in range(0, n_samples, BATCH_SIZE):\n",
    "    end = min(start + BATCH_SIZE, n_samples)\n",
    "    y_batch = input_state_vectors[0:100] # only 100 I/P's for mapping\n",
    "    x_batch = train_inputs[0:100] # only 100 O/P's for mapping\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        x = x_batch[i].reshape(1, -1)  # (1, 1024)\n",
    "        y = y_batch[i].reshape(1, -1)  # (1, 1024)\n",
    "        U = input_mapping_unitary(x, y)\n",
    "        unitary_err = fro_error(U)\n",
    "        loss = mapping_loss(x, y, U)\n",
    "        print(f\"Pair {start + i:06d}: Unitarity error = {unitary_err:.2e}, Mapping RMSE = {loss:.20f}\")\n",
    "        out_path = USER_OUT_DIR / f\"unitary_{start + i:06d}.npy\"\n",
    "        save_and_delete(U, out_path)\n",
    "    del x_batch, y_batch\n",
    "\n",
    "print(\"All unitary matrices saved batchwise in:\", USER_OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc877c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 0 loss tensor(0.0300, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0298, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0296, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0294, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0292, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0290, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0288, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0286, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0284, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0282, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0280, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0278, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0276, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0273, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0271, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0269, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0267, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0265, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0262, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0260, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0258, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0256, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0253, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0251, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0249, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0244, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0242, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0240, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0237, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0235, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0232, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0230, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0228, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0225, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0223, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0221, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0218, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0216, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0214, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0211, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0209, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0206, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0204, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0202, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0199, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0197, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0195, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0192, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0190, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0188, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0185, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0183, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0181, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0178, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0176, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0174, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0172, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0170, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0167, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0165, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0163, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0161, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0159, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0157, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0155, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0153, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0151, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0149, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0147, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0145, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0143, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0141, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0139, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0137, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0136, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0134, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0132, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0130, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0129, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0127, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0125, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0124, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0122, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0121, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0119, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0117, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0116, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0113, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0112, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0110, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0108, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0106, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0105, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0104, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0102, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0100, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0099, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0098, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0096, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0095, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0094, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0092, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0091, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0090, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0089, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0087, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0086, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0085, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0083, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0080, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0078, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0076, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0073, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0068, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0062, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0062, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000000: rec=1.0659e-04, unit_err=5.6801e-15\n",
      "Saved unitary_000000.npy\n",
      "Iterations 0 loss tensor(0.0357, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0355, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0353, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0351, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0348, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0346, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0344, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0342, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0340, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0338, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0336, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0334, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0332, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0330, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0328, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0326, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0324, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0323, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0321, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0319, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0317, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0315, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0314, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0312, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0310, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0308, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0307, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0305, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0303, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0302, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0300, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0298, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0297, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0295, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0293, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0292, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0290, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0288, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0286, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0285, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0283, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0281, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0280, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0278, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0276, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0274, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0272, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0271, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0269, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0267, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0265, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0263, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0261, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0259, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0258, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0256, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0254, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0252, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0250, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0248, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0244, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0242, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0239, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0237, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0235, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0233, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0231, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0229, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0227, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0225, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0222, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0220, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0218, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0216, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0214, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0211, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0209, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0207, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0205, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0203, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0200, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0198, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0196, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0194, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0192, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0189, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0187, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0185, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0183, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0181, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0179, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0176, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0174, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0172, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0170, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0168, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0166, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0164, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0162, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0160, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0158, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0156, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0154, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0152, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0150, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0149, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0147, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0145, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0143, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0141, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0140, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0138, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0136, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0135, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0133, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0131, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0130, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0128, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0126, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0125, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0123, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0122, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0120, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0119, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0118, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0116, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0113, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0112, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0111, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0108, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0107, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0105, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0104, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0103, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0102, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0100, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0099, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0098, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0097, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0096, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0095, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0092, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0091, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0090, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0089, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0087, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0086, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0085, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0083, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0080, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0078, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0076, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0073, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0068, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0062, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(9.7940e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(9.4875e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(9.1903e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(8.9021e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(8.6227e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(8.3517e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(8.0891e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(7.8345e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(7.5877e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(7.3485e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(7.1167e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(6.8920e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(6.6742e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(6.4632e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(6.2587e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(6.0606e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(5.8686e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(5.6826e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(5.5024e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(5.3279e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(5.1588e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(4.9949e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(4.8362e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(4.6825e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(4.5336e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(4.3894e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(4.2498e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(4.1145e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(3.9835e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(3.8566e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(3.7338e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(3.6148e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(3.4996e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(3.3880e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(3.2800e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(3.1754e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(3.0741e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(2.9760e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(2.8810e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(2.7891e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(2.7001e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(2.6139e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(2.5305e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(2.4497e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(2.3715e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(2.2958e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(2.2224e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(2.1515e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(2.0828e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(2.0163e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(1.9519e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(1.8895e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(1.8292e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(1.7708e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(1.7142e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(1.6595e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(1.6065e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(1.5552e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(1.5055e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(1.4574e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(1.4109e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(1.3659e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(1.3222e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(1.2800e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(1.2391e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(1.1996e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(1.1613e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(1.1242e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(1.0883e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(1.0536e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(1.0199e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(9.8737e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(9.5584e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(9.2533e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(8.9579e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(8.6719e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(8.3950e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(8.1270e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(7.8675e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(7.6163e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(7.3732e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(7.1377e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(6.9098e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(6.6892e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(6.4756e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(6.2688e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(6.0686e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(5.8747e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(5.6871e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(5.5054e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(5.3295e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(5.1592e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(4.9943e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(4.8347e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(4.6802e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(4.5306e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(4.3857e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(4.2455e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(4.1097e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(3.9782e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(3.8509e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(3.7277e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(3.6084e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(3.4928e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(3.3810e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(3.2727e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(3.1678e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(3.0663e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(2.9680e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(2.8729e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(2.7807e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(2.6915e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(2.6051e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(2.5215e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(2.4405e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(2.3621e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(2.2862e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(2.2127e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(2.1416e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(2.0727e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(2.0060e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(1.9414e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(1.8788e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(1.8183e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(1.7597e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(1.7029e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(1.6480e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(1.5948e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(1.5433e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(1.4934e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(1.4451e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(1.3984e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(1.3531e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(1.3093e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(1.2669e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(1.2259e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(1.1861e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(1.1476e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(1.1104e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(1.0743e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(1.0394e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(1.0056e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(9.7286e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000001: rec=9.7286e-07, unit_err=5.7450e-15\n",
      "Saved unitary_000001.npy\n",
      "Iterations 0 loss tensor(0.0309, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0307, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0305, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0303, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0301, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0299, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0297, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0295, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0293, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0291, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0289, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0287, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0285, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0283, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0281, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0279, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0277, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0275, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0273, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0270, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0268, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0266, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0264, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0262, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0259, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0257, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0255, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0253, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0250, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0248, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0243, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0241, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0239, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0236, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0234, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0232, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0229, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0227, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0225, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0222, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0220, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0218, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0215, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0213, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0210, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0208, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0206, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0203, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0201, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0199, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0196, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0194, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0192, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0189, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0187, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0185, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0183, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0180, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0178, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0176, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0174, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0171, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0169, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0167, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0165, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0163, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0160, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0158, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0156, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0154, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0152, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0150, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0148, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0146, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0144, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0142, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0140, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0138, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0136, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0134, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0133, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0131, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0129, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0127, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0125, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0124, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0122, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0120, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0118, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0117, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0114, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0112, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0110, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0107, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0106, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0104, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0103, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0100, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0099, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0097, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0096, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0094, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0092, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0090, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0089, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0087, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0086, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0083, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0080, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0076, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0073, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0068, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0062, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(9.8939e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(9.6834e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(9.4770e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(9.2747e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(9.0764e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(8.8819e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(8.6913e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(8.5044e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(8.3212e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(8.1417e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(7.9657e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(7.7933e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(7.6243e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(7.4586e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(7.2963e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(7.1372e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(6.9813e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(6.8286e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(6.6790e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(6.5324e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(6.3887e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(6.2480e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(6.1102e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(5.9751e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(5.8429e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(5.7133e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(5.5864e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(5.4621e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(5.3404e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(5.2212e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(5.1044e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(4.9901e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(4.8781e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(4.7685e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(4.6612e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(4.5561e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(4.4532e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(4.3525e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(4.2538e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(4.1573e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(4.0628e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(3.9703e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(3.8798e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(3.7911e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(3.7044e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(3.6195e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(3.5364e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(3.4551e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(3.3756e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(3.2977e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(3.2215e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(3.1470e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(3.0741e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(3.0027e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(2.9329e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(2.8646e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(2.7978e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(2.7325e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(2.6685e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(2.6060e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(2.5449e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(2.4850e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(2.4265e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(2.3693e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(2.3134e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(2.2587e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(2.2052e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(2.1528e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(2.1017e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(2.0517e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(2.0028e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(1.9550e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(1.9082e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(1.8625e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(1.8179e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(1.7742e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(1.7315e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(1.6898e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(1.6490e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(1.6092e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(1.5703e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(1.5322e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(1.4950e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(1.4587e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(1.4231e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(1.3884e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(1.3545e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(1.3214e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(1.2890e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(1.2574e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(1.2265e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(1.1964e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(1.1669e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(1.1381e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(1.1100e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(1.0825e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(1.0557e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(1.0295e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(1.0039e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(9.7889e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(9.5448e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(9.3065e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(9.0738e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(8.8466e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(8.6248e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(8.4082e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(8.1967e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(7.9903e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(7.7887e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(7.5920e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(7.4000e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(7.2125e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(7.0296e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(6.8510e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(6.6767e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(6.5067e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(6.3408e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(6.1790e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(6.0211e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000002: rec=6.0211e-06, unit_err=5.4221e-15\n",
      "Saved unitary_000002.npy\n",
      "Iterations 0 loss tensor(0.0338, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0336, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0334, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0332, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0330, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0328, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0325, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0323, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0321, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0319, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0317, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0315, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0313, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0312, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0310, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0308, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0306, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0304, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0302, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0300, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0298, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0296, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0294, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0292, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0290, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0288, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0286, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0284, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0282, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0280, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0278, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0275, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0273, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0271, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0269, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0266, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0264, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0262, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0259, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0257, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0254, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0251, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0249, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0244, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0241, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0238, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0235, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0232, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0229, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0227, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0224, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0221, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0218, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0214, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0211, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0208, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0205, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0202, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0199, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0195, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0192, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0189, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0185, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0182, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0179, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0175, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0172, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0169, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0165, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0162, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0159, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0155, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0152, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0149, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0145, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0142, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0139, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0136, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0132, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0129, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0126, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0123, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0120, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0117, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0114, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0111, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0106, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0103, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0100, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0098, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0095, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0090, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0085, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0083, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0068, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(9.8772e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(9.5174e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(9.1706e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(8.8363e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(8.5140e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(8.2035e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(7.9042e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(7.6157e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(7.3377e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(7.0698e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(6.8116e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(6.5628e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(6.3231e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(6.0921e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(5.8695e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(5.6550e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(5.4483e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(5.2492e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(5.0573e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(4.8724e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(4.6943e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(4.5227e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(4.3573e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(4.1980e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(4.0445e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(3.8966e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(3.7541e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(3.6168e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(3.4845e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(3.3571e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(3.2343e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(3.1160e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(3.0021e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(2.8923e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(2.7865e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(2.6845e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(2.5863e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(2.4917e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(2.4006e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(2.3127e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(2.2281e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(2.1466e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(2.0680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(1.9923e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(1.9194e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(1.8491e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(1.7814e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(1.7162e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(1.6533e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(1.5927e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(1.5344e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(1.4782e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(1.4240e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(1.3718e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(1.3215e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(1.2730e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(1.2263e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(1.1813e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(1.1379e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(1.0961e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(1.0558e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(1.0170e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(9.7963e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(9.4360e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(9.0888e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(8.7542e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(8.4318e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(8.1211e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(7.8218e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(7.5333e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(7.2553e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(6.9874e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(6.7293e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(6.4805e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(6.2409e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(6.0099e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(5.7873e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(5.5729e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(5.3662e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(5.1671e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(4.9753e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(4.7904e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(4.6123e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(4.4406e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(4.2752e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(4.1159e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(3.9623e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(3.8144e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(3.6719e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(3.5346e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(3.4022e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(3.2748e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(3.1520e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(3.0337e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(2.9197e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(2.8099e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(2.7041e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(2.6022e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(2.5041e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(2.4095e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(2.3184e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(2.2307e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(2.1462e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(2.0648e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(1.9864e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(1.9110e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(1.8382e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(1.7682e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(1.7008e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(1.6358e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(1.5733e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(1.5131e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(1.4551e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(1.3993e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(1.3455e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(1.2938e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(1.2440e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(1.1960e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(1.1498e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(1.1054e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(1.0626e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(1.0214e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(9.8174e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(9.4358e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(9.0685e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(8.7151e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(8.3750e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(8.0477e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(7.7327e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(7.4297e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(7.1381e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(6.8575e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(6.5876e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(6.3280e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(6.0782e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(5.8379e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(5.6068e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(5.3845e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(5.1707e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(4.9651e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(4.7673e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(4.5772e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(4.3943e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(4.2185e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(4.0495e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(3.8870e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(3.7307e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(3.5806e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(3.4362e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(3.2974e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(3.1641e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(3.0359e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(2.9127e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(2.7944e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(2.6806e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(2.5713e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(2.4664e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(2.3655e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(2.2686e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(2.1755e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(2.0861e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(2.0002e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(1.9178e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(1.8385e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(1.7625e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(1.6895e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(1.6193e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(1.5520e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(1.4874e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(1.4254e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(1.3658e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(1.3087e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(1.2538e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(1.2011e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(1.1506e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(1.1021e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(1.0556e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(1.0110e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(9.6820e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(9.2713e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(8.8774e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(8.4995e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(8.1372e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(7.7897e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(7.4564e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(7.1369e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(6.8305e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(6.5368e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(6.2553e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(5.9854e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(5.7267e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(5.4787e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(5.2411e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(5.0134e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(4.7952e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(4.5861e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(4.3859e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(4.1940e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(4.0102e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(3.8341e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(3.6655e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(3.5040e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(3.3494e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(3.2013e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(3.0596e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(2.9239e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(2.7939e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(2.6695e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(2.5505e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(2.4366e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(2.3275e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(2.2232e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(2.1233e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(2.0278e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(1.9364e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(1.8490e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(1.7654e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(1.6854e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(1.6089e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(1.5358e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(1.4658e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(1.3990e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(1.3350e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(1.2739e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(1.2155e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(1.1596e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(1.1063e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(1.0553e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(1.0065e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(9.5994e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(9.1544e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(8.7294e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(8.3233e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(7.9355e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(7.5650e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(7.2113e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(6.8735e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(6.5509e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(6.2430e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(5.9489e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(5.6683e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(5.4004e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(5.1447e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(4.9007e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(4.6679e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(4.4457e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(4.2338e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(4.0315e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(3.8386e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(3.6547e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(3.4792e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(3.3118e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(3.1522e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(3.0001e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(2.8550e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(2.7167e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(2.5849e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(2.4592e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(2.3394e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(2.2253e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(2.1165e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(2.0129e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(1.9142e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(1.8202e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(1.7306e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(1.6452e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(1.5640e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000003: rec=1.5640e-09, unit_err=5.8190e-15\n",
      "Saved unitary_000003.npy\n",
      "Iterations 0 loss tensor(0.0272, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0270, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0268, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0265, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0263, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0261, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0259, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0257, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0254, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0252, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0250, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0247, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0245, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0243, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0241, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0238, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0236, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0234, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0231, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0229, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0227, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0224, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0222, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0220, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0217, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0215, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0213, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0210, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0208, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0206, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0203, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0201, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0199, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0196, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0194, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0192, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0190, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0187, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0185, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0183, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0181, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0178, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0176, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0174, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0172, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0170, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0168, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0165, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0163, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0161, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0159, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0157, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0155, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0153, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0151, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0149, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0147, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0145, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0143, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0141, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0139, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0137, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0135, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0133, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0131, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0130, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0128, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0126, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0124, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0122, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0121, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0119, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0117, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0114, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0112, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0110, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0107, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0105, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0104, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0102, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0100, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0099, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0097, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0096, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0094, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0091, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0090, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0087, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0086, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0083, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0080, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0076, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0062, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(9.9377e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(9.6512e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(9.3726e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(9.1017e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(8.8384e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(8.5823e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(8.3334e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(8.0915e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(7.8563e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(7.6276e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(7.4055e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(7.1895e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(6.9796e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(6.7757e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(6.5774e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(6.3848e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(6.1977e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(6.0158e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(5.8392e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(5.6675e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(5.5007e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(5.3387e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(5.1813e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(5.0284e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(4.8799e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(4.7356e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(4.5955e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(4.4594e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(4.3272e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(4.1988e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(4.0741e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(3.9531e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(3.8355e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(3.7213e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(3.6104e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(3.5027e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(3.3982e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(3.2967e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(3.1981e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(3.1025e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(3.0096e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(2.9194e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(2.8318e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(2.7468e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(2.6643e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(2.5842e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(2.5065e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(2.4310e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(2.3577e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(2.2866e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(2.2176e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(2.1506e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(2.0856e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(2.0225e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(1.9613e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(1.9019e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(1.8442e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(1.7882e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(1.7339e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(1.6812e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(1.6301e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(1.5805e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(1.5323e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(1.4856e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(1.4403e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(1.3963e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(1.3536e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(1.3122e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(1.2721e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(1.2331e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(1.1953e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(1.1586e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(1.1230e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(1.0885e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(1.0550e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(1.0226e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(9.9105e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(9.6049e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(9.3085e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(9.0210e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(8.7421e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(8.4716e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(8.2093e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(7.9549e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(7.7081e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(7.4688e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(7.2367e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(7.0117e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(6.7934e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(6.5818e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(6.3765e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(6.1775e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(5.9845e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(5.7974e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(5.6159e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(5.4400e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(5.2694e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(5.1040e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(4.9437e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(4.7883e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(4.6375e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(4.4914e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(4.3498e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(4.2125e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(4.0794e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(3.9504e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(3.8253e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(3.7041e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(3.5866e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(3.4727e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(3.3623e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(3.2553e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(3.1516e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(3.0511e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(2.9537e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(2.8593e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(2.7679e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(2.6792e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(2.5933e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(2.5101e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(2.4295e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(2.3514e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(2.2757e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(2.2023e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(2.1312e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(2.0624e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(1.9957e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(1.9311e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(1.8685e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(1.8079e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(1.7492e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(1.6923e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(1.6372e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(1.5838e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(1.5321e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(1.4821e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(1.4336e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(1.3866e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(1.3412e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(1.2971e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(1.2545e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(1.2132e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(1.1733e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(1.1346e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(1.0971e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(1.0608e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(1.0257e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(9.9171e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(9.5880e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(9.2694e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(8.9610e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(8.6626e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(8.3737e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(8.0940e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(7.8234e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(7.5615e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(7.3081e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(7.0628e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(6.8255e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(6.5958e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(6.3736e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(6.1586e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(5.9506e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(5.7494e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(5.5547e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(5.3664e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(5.1842e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(5.0079e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(4.8375e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(4.6726e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(4.5131e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(4.3589e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(4.2098e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(4.0655e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(3.9261e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(3.7912e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(3.6608e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(3.5347e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(3.4129e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(3.2952e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(3.1815e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(3.0719e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(2.9665e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(2.8657e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(2.7697e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(2.6774e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(2.5853e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(2.4903e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(2.3971e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(2.3134e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(2.2369e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(2.1592e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(2.0792e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(2.0044e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(1.9366e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(1.8694e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(1.8007e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(1.7359e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(1.6764e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(1.6174e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(1.5580e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(1.5024e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(1.4503e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(1.3983e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(1.3471e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(1.2992e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(1.2535e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(1.2080e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(1.1640e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(1.1226e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(1.0825e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(1.0430e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(1.0051e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(9.6914e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(9.3410e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(8.9981e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(8.6713e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000004: rec=8.6713e-08, unit_err=5.2860e-15\n",
      "Saved unitary_000004.npy\n",
      "Iterations 0 loss tensor(0.0281, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0279, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0277, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0275, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0273, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0271, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0269, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0267, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0265, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0263, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0261, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0259, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0257, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0255, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0253, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0251, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0249, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0247, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0245, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0243, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0241, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0238, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0236, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0234, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0232, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0230, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0228, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0226, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0224, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0222, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0219, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0217, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0215, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0213, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0211, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0209, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0206, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0204, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0202, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0200, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0198, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0195, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0193, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0191, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0189, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0187, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0184, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0182, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0180, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0178, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0176, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0173, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0171, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0169, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0167, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0165, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0163, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0160, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0158, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0156, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0154, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0152, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0150, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0148, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0146, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0144, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0142, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0140, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0137, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0135, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0134, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0132, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0130, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0128, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0126, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0124, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0122, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0120, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0118, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0116, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0113, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0111, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0108, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0106, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0104, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0102, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0099, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0097, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0096, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0094, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0091, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0090, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0087, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0085, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0078, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0076, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(9.9125e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(9.6431e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(9.3805e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(9.1243e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(8.8747e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(8.6312e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(8.3940e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(8.1627e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(7.9373e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(7.7176e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(7.5035e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(7.2949e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(7.0916e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(6.8936e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(6.7007e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(6.5127e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(6.3297e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(6.1514e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(5.9777e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(5.8086e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(5.6439e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(5.4835e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(5.3274e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(5.1754e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(5.0274e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(4.8834e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(4.7432e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(4.6067e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(4.4739e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(4.3447e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(4.2191e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(4.0968e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(3.9776e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(3.8617e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(3.7490e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(3.6395e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(3.5328e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(3.4291e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(3.3283e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(3.2302e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(3.1348e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(3.0422e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(2.9520e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(2.8643e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(2.7792e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(2.6963e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(2.6158e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(2.5376e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(2.4615e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(2.3876e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(2.3158e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(2.2460e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(2.1782e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(2.1124e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(2.0484e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(1.9862e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(1.9258e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(1.8672e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(1.8102e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(1.7549e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(1.7012e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(1.6490e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(1.5984e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(1.5492e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(1.5014e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(1.4551e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(1.4101e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(1.3664e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(1.3240e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(1.2829e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(1.2429e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(1.2042e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(1.1666e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(1.1301e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(1.0947e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(1.0603e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(1.0270e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(9.9468e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(9.6332e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(9.3291e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(9.0340e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(8.7478e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(8.4703e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(8.2012e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(7.9402e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(7.6872e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(7.4418e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(7.2039e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(6.9733e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(6.7497e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(6.5330e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(6.3229e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(6.1193e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(5.9219e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(5.7306e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(5.5453e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(5.3656e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(5.1916e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(5.0229e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(4.8595e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(4.7012e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(4.5478e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(4.3992e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(4.2553e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(4.1159e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(3.9808e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(3.8501e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(3.7234e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(3.6007e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(3.4819e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(3.3669e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(3.2555e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(3.1477e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(3.0432e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(2.9422e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(2.8443e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(2.7496e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(2.6579e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(2.5691e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(2.4832e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(2.4000e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(2.3196e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(2.2417e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(2.1663e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(2.0934e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(2.0228e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(1.9546e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(1.8885e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(1.8246e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(1.7628e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(1.7030e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(1.6451e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(1.5892e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(1.5351e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(1.4827e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(1.4321e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(1.3831e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(1.3358e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(1.2900e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(1.2457e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(1.2029e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(1.1615e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(1.1215e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(1.0829e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(1.0455e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(1.0093e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(9.7437e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(9.4059e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(9.0795e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(8.7640e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(8.4590e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(8.1644e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(7.8796e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(7.6045e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(7.3386e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(7.0817e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(6.8335e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(6.5936e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(6.3620e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(6.1382e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(5.9220e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(5.7131e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(5.5114e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(5.3166e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(5.1284e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(4.9467e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(4.7712e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(4.6017e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(4.4380e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(4.2800e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(4.1274e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(3.9801e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(3.8379e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(3.7005e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(3.5680e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(3.4400e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(3.3165e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(3.1973e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(3.0822e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(2.9711e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(2.8639e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(2.7604e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(2.6606e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(2.5643e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(2.4713e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(2.3816e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(2.2951e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(2.2116e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(2.1310e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(2.0533e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(1.9783e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(1.9060e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(1.8362e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(1.7690e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(1.7041e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(1.6415e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(1.5811e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(1.5229e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(1.4668e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(1.4127e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(1.3605e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(1.3101e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000005: rec=1.3101e-07, unit_err=5.4686e-15\n",
      "Saved unitary_000005.npy\n",
      "Iterations 0 loss tensor(0.0286, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0284, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0282, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0280, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0278, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0276, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0274, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0272, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0270, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0268, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0266, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0264, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0262, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0260, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0258, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0256, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0254, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0252, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0250, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0248, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0244, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0242, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0240, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0238, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0235, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0233, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0231, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0229, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0227, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0225, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0223, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0221, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0219, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0216, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0214, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0212, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0210, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0208, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0206, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0204, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0202, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0200, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0197, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0195, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0193, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0191, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0189, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0187, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0185, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0183, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0181, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0179, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0177, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0175, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0173, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0171, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0169, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0167, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0165, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0163, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0162, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0160, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0158, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0156, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0154, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0152, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0151, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0149, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0147, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0145, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0144, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0142, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0140, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0139, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0137, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0135, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0134, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0132, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0131, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0129, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0128, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0126, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0125, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0123, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0122, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0120, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0119, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0117, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0116, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0113, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0112, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0111, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0108, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0107, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0105, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0104, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0103, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0102, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0100, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0099, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0098, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0097, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0096, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0094, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0092, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0091, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0090, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0089, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0087, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0086, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0085, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0083, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0080, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0078, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0076, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0073, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0068, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0062, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(9.9330e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(9.5899e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(9.2575e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(8.9355e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(8.6236e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(8.3216e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(8.0292e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(7.7462e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(7.4722e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(7.2072e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(6.9507e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(6.7026e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(6.4627e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(6.2308e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(6.0065e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(5.7897e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(5.5802e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(5.3778e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(5.1822e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(4.9933e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(4.8108e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(4.6346e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(4.4645e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(4.3002e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(4.1417e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(3.9888e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(3.8411e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(3.6987e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(3.5613e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(3.4288e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(3.3011e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(3.1778e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(3.0590e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(2.9445e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(2.8341e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(2.7277e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(2.6252e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(2.5264e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(2.4313e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(2.3396e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(2.2513e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(2.1663e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(2.0844e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(2.0055e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(1.9296e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(1.8565e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(1.7861e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(1.7183e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(1.6531e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(1.5904e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(1.5300e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(1.4719e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(1.4160e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(1.3622e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(1.3105e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(1.2607e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(1.2128e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(1.1668e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(1.1224e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(1.0798e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(1.0389e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(9.9945e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(9.6154e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(9.2510e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(8.9005e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(8.5635e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(8.2395e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(7.9279e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(7.6284e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(7.3403e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(7.0634e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(6.7971e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(6.5412e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(6.2950e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(6.0584e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(5.8309e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(5.6122e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(5.4019e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(5.1997e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(5.0053e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(4.8185e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(4.6388e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(4.4660e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(4.2999e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(4.1401e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(3.9865e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(3.8388e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(3.6968e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(3.5602e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(3.4289e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(3.3025e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(3.1811e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(3.0642e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(2.9518e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(2.8437e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(2.7397e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(2.6397e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(2.5435e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(2.4509e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(2.3618e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(2.2761e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(2.1937e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(2.1143e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(2.0379e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(1.9644e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(1.8937e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(1.8256e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(1.7601e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(1.6970e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(1.6362e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(1.5778e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(1.5215e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(1.4673e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(1.4151e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(1.3648e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(1.3164e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(1.2697e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(1.2248e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(1.1815e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(1.1399e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(1.0997e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(1.0610e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(1.0237e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(9.8780e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(9.5318e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(9.1981e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(8.8764e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(8.5664e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(8.2676e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(7.9795e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(7.7017e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(7.4340e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(7.1757e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(6.9268e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(6.6866e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(6.4551e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(6.2317e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(6.0163e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(5.8085e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(5.6081e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(5.4147e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(5.2281e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(5.0481e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(4.8744e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(4.7068e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(4.5451e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(4.3890e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(4.2384e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(4.0930e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(3.9527e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(3.8173e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(3.6866e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(3.5604e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(3.4385e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(3.3209e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(3.2074e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(3.0977e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(2.9919e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(2.8897e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(2.7910e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(2.6956e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(2.6036e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(2.5147e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(2.4289e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(2.3460e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(2.2659e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(2.1886e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(2.1139e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000006: rec=2.1139e-07, unit_err=5.3701e-15\n",
      "Saved unitary_000006.npy\n",
      "Iterations 0 loss tensor(0.0344, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0342, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0339, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0337, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0335, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0332, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0330, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0328, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0326, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0323, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0321, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0319, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0317, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0315, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0313, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0310, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0308, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0306, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0304, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0302, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0300, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0298, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0296, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0294, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0291, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0289, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0287, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0285, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0283, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0280, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0278, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0276, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0273, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0271, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0269, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0266, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0264, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0261, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0259, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0256, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0254, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0251, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0248, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0243, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0240, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0237, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0234, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0231, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0228, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0225, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0222, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0219, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0216, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0213, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0210, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0207, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0203, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0200, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0197, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0194, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0190, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0187, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0184, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0180, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0177, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0174, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0170, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0167, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0163, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0160, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0157, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0153, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0150, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0147, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0143, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0140, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0137, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0134, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0130, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0127, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0124, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0121, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0118, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0112, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0106, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0103, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0100, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0097, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0095, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0092, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0089, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0087, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0080, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0073, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(9.9518e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(9.6647e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(9.3859e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(9.1149e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(8.8517e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(8.5960e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(8.3476e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(8.1062e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(7.8717e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(7.6439e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(7.4225e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(7.2074e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(6.9984e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(6.7953e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(6.5980e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(6.4062e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(6.2199e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(6.0388e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(5.8629e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(5.6919e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(5.5258e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(5.3644e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(5.2075e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(5.0550e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(4.9069e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(4.7629e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(4.6230e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(4.4870e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(4.3549e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(4.2265e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(4.1018e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(3.9805e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(3.8627e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(3.7482e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(3.6370e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(3.5289e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(3.4239e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(3.3218e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(3.2226e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(3.1263e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(3.0326e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(2.9417e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(2.8533e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(2.7674e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(2.6840e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(2.6029e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(2.5242e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(2.4477e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(2.3734e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(2.3013e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(2.2311e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(2.1631e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(2.0969e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(2.0327e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(1.9703e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(1.9097e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(1.8509e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(1.7937e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(1.7383e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(1.6844e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(1.6321e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(1.5813e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(1.5320e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(1.4842e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(1.4377e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(1.3926e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(1.3489e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(1.3064e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(1.2651e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(1.2251e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(1.1863e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(1.1486e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(1.1121e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(1.0766e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(1.0422e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(1.0088e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(9.7643e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(9.4502e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(9.1456e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(8.8501e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(8.5636e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(8.2857e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(8.0163e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(7.7551e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(7.5018e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(7.2563e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(7.0183e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(6.7875e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(6.5639e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(6.3472e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(6.1372e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(5.9336e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(5.7364e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(5.5453e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(5.3602e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(5.1809e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(5.0072e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(4.8389e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(4.6759e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(4.5181e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(4.3653e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(4.2173e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(4.0740e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(3.9353e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(3.8010e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(3.6710e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(3.5452e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(3.4234e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(3.3055e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(3.1915e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(3.0811e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(2.9743e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(2.8711e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(2.7711e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(2.6745e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(2.5810e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(2.4906e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(2.4031e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(2.3186e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(2.2368e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(2.1578e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(2.0813e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(2.0075e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(1.9361e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(1.8671e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(1.8004e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(1.7359e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(1.6737e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(1.6135e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(1.5554e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(1.4992e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(1.4450e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(1.3926e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(1.3420e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(1.2931e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(1.2459e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(1.2004e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(1.1564e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(1.1139e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(1.0729e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(1.0334e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(9.9518e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(9.5833e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(9.2277e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(8.8846e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(8.5536e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(8.2342e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(7.9262e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(7.6290e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(7.3424e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(7.0660e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(6.7994e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(6.5424e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(6.2946e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(6.0557e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(5.8254e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(5.6034e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(5.3894e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(5.1832e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(4.9844e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(4.7929e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(4.6084e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(4.4306e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(4.2594e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(4.0944e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(3.9355e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(3.7825e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(3.6351e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(3.4932e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(3.3566e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(3.2250e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(3.0984e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(2.9764e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(2.8591e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(2.7461e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(2.6374e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(2.5328e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(2.4322e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(2.3354e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(2.2422e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(2.1526e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(2.0664e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(1.9835e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(1.9037e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(1.8270e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(1.7533e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(1.6824e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(1.6142e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(1.5487e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(1.4857e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(1.4252e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(1.3670e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(1.3111e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(1.2574e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(1.2057e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(1.1561e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(1.1085e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(1.0627e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(1.0187e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(9.7651e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(9.3596e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(8.9702e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(8.5963e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(8.2373e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(7.8926e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(7.5618e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(7.2442e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(6.9394e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(6.6469e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(6.3662e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(6.0969e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(5.8385e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(5.5905e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(5.3527e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(5.1245e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(4.9057e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(4.6958e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(4.4946e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(4.3016e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(4.1166e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(3.9392e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(3.7691e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(3.6061e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(3.4498e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(3.3001e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(3.1566e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(3.0191e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(2.8873e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(2.7610e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(2.6401e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(2.5243e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(2.4133e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(2.3070e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(2.2052e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(2.1078e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(2.0144e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(1.9251e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(1.8395e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(1.7576e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(1.6793e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(1.6042e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(1.5324e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(1.4637e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(1.3980e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(1.3351e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(1.2749e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(1.2174e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(1.1623e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(1.1096e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(1.0593e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(1.0111e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(9.6503e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(9.2099e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(8.7889e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(8.3864e-09, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000007: rec=8.3864e-09, unit_err=5.2068e-15\n",
      "Saved unitary_000007.npy\n",
      "Iterations 0 loss tensor(0.0309, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0307, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0305, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0304, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0302, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0300, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0298, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0296, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0294, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0293, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0291, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0289, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0287, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0285, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0283, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0281, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0279, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0278, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0276, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0274, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0272, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0270, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0268, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0266, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0264, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0262, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0260, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0258, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0256, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0254, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0252, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0250, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0248, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0244, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0242, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0240, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0238, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0236, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0233, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0231, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0229, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0227, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0225, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0223, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0221, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0219, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0217, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0215, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0213, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0211, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0209, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0207, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0205, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0203, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0201, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0199, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0197, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0195, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0193, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0191, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0189, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0188, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0186, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0184, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0182, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0180, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0178, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0176, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0175, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0173, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0171, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0169, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0168, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0166, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0164, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0162, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0161, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0159, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0157, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0156, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0154, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0152, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0151, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0149, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0147, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0146, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0144, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0143, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0141, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0139, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0138, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0136, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0135, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0133, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0132, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0130, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0129, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0127, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0126, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0124, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0123, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0122, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0120, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0119, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0117, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0116, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0114, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0113, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0112, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0110, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0108, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0106, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0105, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0104, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0102, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0100, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0098, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0097, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0096, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0094, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0092, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0091, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0090, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0087, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0086, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0085, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0080, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0078, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0076, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0073, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0071, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0068, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0065, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0062, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(9.8346e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(9.5838e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(9.3392e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(9.1007e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(8.8679e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(8.6410e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(8.4194e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(8.2031e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(7.9923e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(7.7868e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(7.5862e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(7.3905e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(7.1998e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(7.0137e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(6.8322e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(6.6553e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(6.4826e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(6.3143e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(6.1502e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(5.9901e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(5.8341e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(5.6819e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(5.5335e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(5.3888e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(5.2478e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(5.1102e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(4.9762e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(4.8454e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(4.7180e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(4.5937e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(4.4726e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(4.3546e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(4.2395e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(4.1273e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(4.0180e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(3.9114e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(3.8075e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(3.7063e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(3.6076e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(3.5115e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(3.4178e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(3.3265e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(3.2375e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(3.1508e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(3.0664e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(2.9841e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(2.9038e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(2.8256e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(2.7495e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(2.6753e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(2.6031e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(2.5326e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(2.4640e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(2.3972e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(2.3321e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(2.2687e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(2.2070e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(2.1468e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(2.0883e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(2.0312e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(1.9757e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(1.9216e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(1.8689e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(1.8176e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(1.7676e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(1.7189e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(1.6716e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(1.6255e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(1.5806e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(1.5368e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(1.4943e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(1.4528e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(1.4125e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(1.3732e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(1.3350e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(1.2978e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(1.2616e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(1.2264e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(1.1920e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(1.1587e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(1.1262e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(1.0946e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(1.0638e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(1.0338e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(1.0047e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(9.7635e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(9.4877e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(9.2193e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(8.9582e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(8.7042e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(8.4571e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(8.2166e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(7.9828e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(7.7553e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(7.5340e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(7.3188e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(7.1094e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(6.9058e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(6.7078e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(6.5152e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(6.3280e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(6.1459e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(5.9689e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(5.7969e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(5.6298e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(5.4674e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(5.3093e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(5.1550e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(5.0047e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(4.8590e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(4.7179e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(4.5808e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(4.4469e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(4.3167e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(4.1905e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(4.0681e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(3.9488e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(3.8326e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(3.7200e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(3.6107e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(3.5043e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(3.4008e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(3.3004e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(3.2029e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(3.1081e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(3.0159e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(2.9264e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(2.8397e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(2.7555e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(2.6742e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(2.5958e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(2.5192e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(2.4423e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(2.3675e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(2.2978e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(2.2296e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(2.1611e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(2.0962e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(2.0340e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(1.9717e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(1.9121e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(1.8550e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(1.7982e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(1.7437e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(1.6913e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(1.6394e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(1.5896e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(1.5415e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(1.4941e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(1.4487e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(1.4045e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(1.3613e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(1.3198e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(1.2793e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(1.2398e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(1.2019e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(1.1648e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(1.1288e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000008: rec=1.1288e-06, unit_err=5.2327e-15\n",
      "Saved unitary_000008.npy\n",
      "Iterations 0 loss tensor(0.0261, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 1 loss tensor(0.0260, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 2 loss tensor(0.0258, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 3 loss tensor(0.0256, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 4 loss tensor(0.0254, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 5 loss tensor(0.0252, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 6 loss tensor(0.0251, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 7 loss tensor(0.0249, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 8 loss tensor(0.0247, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 9 loss tensor(0.0245, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 10 loss tensor(0.0243, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 11 loss tensor(0.0241, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 12 loss tensor(0.0239, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 13 loss tensor(0.0238, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 14 loss tensor(0.0236, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 15 loss tensor(0.0234, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 16 loss tensor(0.0232, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 17 loss tensor(0.0230, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 18 loss tensor(0.0228, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 19 loss tensor(0.0226, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 20 loss tensor(0.0224, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 21 loss tensor(0.0222, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 22 loss tensor(0.0220, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 23 loss tensor(0.0218, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 24 loss tensor(0.0216, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 25 loss tensor(0.0214, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 26 loss tensor(0.0212, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 27 loss tensor(0.0210, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 28 loss tensor(0.0208, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 29 loss tensor(0.0206, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 30 loss tensor(0.0204, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 31 loss tensor(0.0202, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 32 loss tensor(0.0200, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 33 loss tensor(0.0198, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 34 loss tensor(0.0195, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 35 loss tensor(0.0193, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 36 loss tensor(0.0191, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 37 loss tensor(0.0189, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 38 loss tensor(0.0187, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 39 loss tensor(0.0185, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 40 loss tensor(0.0183, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 41 loss tensor(0.0181, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 42 loss tensor(0.0178, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 43 loss tensor(0.0176, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 44 loss tensor(0.0174, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 45 loss tensor(0.0172, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 46 loss tensor(0.0170, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 47 loss tensor(0.0168, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 48 loss tensor(0.0166, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 49 loss tensor(0.0163, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 50 loss tensor(0.0161, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 51 loss tensor(0.0159, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 52 loss tensor(0.0157, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 53 loss tensor(0.0155, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 54 loss tensor(0.0153, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 55 loss tensor(0.0151, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 56 loss tensor(0.0149, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 57 loss tensor(0.0146, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 58 loss tensor(0.0144, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 59 loss tensor(0.0142, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 60 loss tensor(0.0140, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 61 loss tensor(0.0138, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 62 loss tensor(0.0136, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 63 loss tensor(0.0134, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 64 loss tensor(0.0132, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 65 loss tensor(0.0130, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 66 loss tensor(0.0128, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 67 loss tensor(0.0126, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 68 loss tensor(0.0124, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 69 loss tensor(0.0122, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 70 loss tensor(0.0120, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 71 loss tensor(0.0119, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 72 loss tensor(0.0117, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 73 loss tensor(0.0115, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 74 loss tensor(0.0113, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 75 loss tensor(0.0111, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 76 loss tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 77 loss tensor(0.0108, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 78 loss tensor(0.0106, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 79 loss tensor(0.0104, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 80 loss tensor(0.0102, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 81 loss tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 82 loss tensor(0.0099, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 83 loss tensor(0.0097, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 84 loss tensor(0.0096, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 85 loss tensor(0.0094, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 86 loss tensor(0.0093, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 87 loss tensor(0.0091, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 88 loss tensor(0.0089, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 89 loss tensor(0.0088, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 90 loss tensor(0.0086, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 91 loss tensor(0.0085, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 92 loss tensor(0.0084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 93 loss tensor(0.0082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 94 loss tensor(0.0081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 95 loss tensor(0.0079, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 96 loss tensor(0.0078, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 97 loss tensor(0.0077, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 98 loss tensor(0.0075, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 99 loss tensor(0.0074, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 100 loss tensor(0.0073, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 101 loss tensor(0.0072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 102 loss tensor(0.0070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 103 loss tensor(0.0069, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 104 loss tensor(0.0068, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 105 loss tensor(0.0067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 106 loss tensor(0.0066, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 107 loss tensor(0.0064, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 108 loss tensor(0.0063, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 109 loss tensor(0.0062, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 110 loss tensor(0.0061, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 111 loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 112 loss tensor(0.0059, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 113 loss tensor(0.0058, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 114 loss tensor(0.0057, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 115 loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 116 loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 117 loss tensor(0.0054, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 118 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 119 loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 120 loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 121 loss tensor(0.0051, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 122 loss tensor(0.0050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 123 loss tensor(0.0049, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 124 loss tensor(0.0048, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 125 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 126 loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 127 loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 128 loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 129 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 130 loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 131 loss tensor(0.0043, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 132 loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 133 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 134 loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 135 loss tensor(0.0040, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 136 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 137 loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 138 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 139 loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 140 loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 141 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 142 loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 143 loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 144 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 145 loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 146 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 147 loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 148 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 149 loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 150 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 151 loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 152 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 153 loss tensor(0.0030, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 154 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 155 loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 156 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 157 loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 158 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 159 loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 160 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 161 loss tensor(0.0026, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 162 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 163 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 164 loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 165 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 166 loss tensor(0.0024, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 167 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 168 loss tensor(0.0023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 169 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 170 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 171 loss tensor(0.0022, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 172 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 173 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 174 loss tensor(0.0021, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 175 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 176 loss tensor(0.0020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 177 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 178 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 179 loss tensor(0.0019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 180 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 181 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 182 loss tensor(0.0018, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 183 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 184 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 185 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 186 loss tensor(0.0017, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 187 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 188 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 189 loss tensor(0.0016, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 190 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 191 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 192 loss tensor(0.0015, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 193 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 194 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 195 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 196 loss tensor(0.0014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 197 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 198 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 199 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 200 loss tensor(0.0013, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 201 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 202 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 203 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 204 loss tensor(0.0012, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 205 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 206 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 207 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 208 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 209 loss tensor(0.0011, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 210 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 211 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 212 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 213 loss tensor(0.0010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 214 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 215 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 216 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 217 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 218 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 219 loss tensor(0.0009, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 220 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 221 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 222 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 223 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 224 loss tensor(0.0008, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 225 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 226 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 227 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 228 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 229 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 230 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 231 loss tensor(0.0007, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 232 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 233 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 234 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 235 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 236 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 237 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 238 loss tensor(0.0006, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 239 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 240 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 241 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 242 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 243 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 244 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 245 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 246 loss tensor(0.0005, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 247 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 248 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 249 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 250 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 251 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 252 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 253 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 254 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 255 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 256 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 257 loss tensor(0.0004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 258 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 259 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 260 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 261 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 262 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 263 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 264 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 265 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 266 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 267 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 268 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 269 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 270 loss tensor(0.0003, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 271 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 272 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 273 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 274 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 275 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 276 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 277 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 278 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 279 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 280 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 281 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 282 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 283 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 284 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 285 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 286 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 287 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 288 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 289 loss tensor(0.0002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 290 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 291 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 292 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 293 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 294 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 295 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 296 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 297 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 298 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 299 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 300 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 301 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 302 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 303 loss tensor(0.0001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 304 loss tensor(9.9529e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 305 loss tensor(9.6747e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 306 loss tensor(9.4037e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 307 loss tensor(9.1399e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 308 loss tensor(8.8829e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 309 loss tensor(8.6328e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 310 loss tensor(8.3893e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 311 loss tensor(8.1522e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 312 loss tensor(7.9214e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 313 loss tensor(7.6968e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 314 loss tensor(7.4781e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 315 loss tensor(7.2654e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 316 loss tensor(7.0583e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 317 loss tensor(6.8568e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 318 loss tensor(6.6608e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 319 loss tensor(6.4700e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 320 loss tensor(6.2844e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 321 loss tensor(6.1039e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 322 loss tensor(5.9283e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 323 loss tensor(5.7575e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 324 loss tensor(5.5913e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 325 loss tensor(5.4298e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 326 loss tensor(5.2727e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 327 loss tensor(5.1200e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 328 loss tensor(4.9713e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 329 loss tensor(4.8267e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 330 loss tensor(4.6862e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 331 loss tensor(4.5497e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 332 loss tensor(4.4168e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 333 loss tensor(4.2877e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 334 loss tensor(4.1623e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 335 loss tensor(4.0403e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 336 loss tensor(3.9217e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 337 loss tensor(3.8066e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 338 loss tensor(3.6946e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 339 loss tensor(3.5858e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 340 loss tensor(3.4801e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 341 loss tensor(3.3773e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 342 loss tensor(3.2774e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 343 loss tensor(3.1805e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 344 loss tensor(3.0862e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 345 loss tensor(2.9947e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 346 loss tensor(2.9057e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 347 loss tensor(2.8193e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 348 loss tensor(2.7354e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 349 loss tensor(2.6539e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 350 loss tensor(2.5747e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 351 loss tensor(2.4978e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 352 loss tensor(2.4231e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 353 loss tensor(2.3506e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 354 loss tensor(2.2802e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 355 loss tensor(2.2118e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 356 loss tensor(2.1453e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 357 loss tensor(2.0809e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 358 loss tensor(2.0182e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 359 loss tensor(1.9575e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 360 loss tensor(1.8984e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 361 loss tensor(1.8412e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 362 loss tensor(1.7855e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 363 loss tensor(1.7315e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 364 loss tensor(1.6791e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 365 loss tensor(1.6283e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 366 loss tensor(1.5789e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 367 loss tensor(1.5309e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 368 loss tensor(1.4844e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 369 loss tensor(1.4393e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 370 loss tensor(1.3955e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 371 loss tensor(1.3530e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 372 loss tensor(1.3117e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 373 loss tensor(1.2717e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 374 loss tensor(1.2328e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 375 loss tensor(1.1951e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 376 loss tensor(1.1585e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 377 loss tensor(1.1230e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 378 loss tensor(1.0886e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 379 loss tensor(1.0552e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 380 loss tensor(1.0228e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 381 loss tensor(9.9134e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 382 loss tensor(9.6084e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 383 loss tensor(9.3125e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 384 loss tensor(9.0255e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 385 loss tensor(8.7471e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 386 loss tensor(8.4770e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 387 loss tensor(8.2151e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 388 loss tensor(7.9611e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 389 loss tensor(7.7147e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 390 loss tensor(7.4757e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 391 loss tensor(7.2440e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 392 loss tensor(7.0192e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 393 loss tensor(6.8012e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 394 loss tensor(6.5899e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 395 loss tensor(6.3849e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 396 loss tensor(6.1861e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 397 loss tensor(5.9934e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 398 loss tensor(5.8066e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 399 loss tensor(5.6254e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 400 loss tensor(5.4497e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 401 loss tensor(5.2793e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 402 loss tensor(5.1142e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 403 loss tensor(4.9541e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 404 loss tensor(4.7989e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 405 loss tensor(4.6484e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 406 loss tensor(4.5025e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 407 loss tensor(4.3611e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 408 loss tensor(4.2240e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 409 loss tensor(4.0912e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 410 loss tensor(3.9624e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 411 loss tensor(3.8375e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 412 loss tensor(3.7165e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 413 loss tensor(3.5992e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 414 loss tensor(3.4855e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 415 loss tensor(3.3753e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 416 loss tensor(3.2686e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 417 loss tensor(3.1651e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 418 loss tensor(3.0648e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 419 loss tensor(2.9676e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 420 loss tensor(2.8734e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 421 loss tensor(2.7821e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 422 loss tensor(2.6937e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 423 loss tensor(2.6080e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 424 loss tensor(2.5249e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 425 loss tensor(2.4444e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 426 loss tensor(2.3665e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 427 loss tensor(2.2909e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 428 loss tensor(2.2177e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 429 loss tensor(2.1468e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 430 loss tensor(2.0781e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 431 loss tensor(2.0115e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 432 loss tensor(1.9471e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 433 loss tensor(1.8846e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 434 loss tensor(1.8241e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 435 loss tensor(1.7654e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 436 loss tensor(1.7087e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 437 loss tensor(1.6536e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 438 loss tensor(1.6004e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 439 loss tensor(1.5487e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 440 loss tensor(1.4988e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 441 loss tensor(1.4503e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 442 loss tensor(1.4034e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 443 loss tensor(1.3580e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 444 loss tensor(1.3140e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 445 loss tensor(1.2714e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 446 loss tensor(1.2302e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 447 loss tensor(1.1902e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 448 loss tensor(1.1515e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 449 loss tensor(1.1140e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 450 loss tensor(1.0778e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 451 loss tensor(1.0426e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 452 loss tensor(1.0086e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 453 loss tensor(9.7567e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 454 loss tensor(9.4378e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 455 loss tensor(9.1290e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 456 loss tensor(8.8300e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 457 loss tensor(8.5406e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 458 loss tensor(8.2603e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 459 loss tensor(7.9890e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 460 loss tensor(7.7264e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 461 loss tensor(7.4722e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 462 loss tensor(7.2261e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 463 loss tensor(6.9878e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 464 loss tensor(6.7572e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 465 loss tensor(6.5340e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 466 loss tensor(6.3180e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 467 loss tensor(6.1089e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 468 loss tensor(5.9065e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 469 loss tensor(5.7106e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 470 loss tensor(5.5210e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 471 loss tensor(5.3376e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 472 loss tensor(5.1600e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 473 loss tensor(4.9882e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 474 loss tensor(4.8220e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 475 loss tensor(4.6611e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 476 loss tensor(4.5054e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 477 loss tensor(4.3548e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 478 loss tensor(4.2090e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 479 loss tensor(4.0680e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 480 loss tensor(3.9316e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 481 loss tensor(3.7996e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 482 loss tensor(3.6719e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 483 loss tensor(3.5484e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 484 loss tensor(3.4289e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 485 loss tensor(3.3133e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 486 loss tensor(3.2015e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 487 loss tensor(3.0933e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 488 loss tensor(2.9887e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 489 loss tensor(2.8875e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 490 loss tensor(2.7896e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 491 loss tensor(2.6950e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 492 loss tensor(2.6034e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 493 loss tensor(2.5149e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 494 loss tensor(2.4293e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 495 loss tensor(2.3465e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 496 loss tensor(2.2664e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 497 loss tensor(2.1890e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 498 loss tensor(2.1141e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "Iterations 499 loss tensor(2.0418e-07, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "#000009: rec=2.0418e-07, unit_err=5.7670e-15\n",
      "Saved unitary_000009.npy\n",
      "Done. Saved all unitaries to: C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_Conjugate_Gradients\n"
     ]
    }
   ],
   "source": [
    "# Input Data Mapping on Quantum Computer by Conjugate Gradients -\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "USER_OUT_DIR = Path(r\"C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_Conjugate_Gradients\")\n",
    "USER_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE       = 1000      \n",
    "MAX_ITERS        = 500    \n",
    "LR               = 1e-2   \n",
    "FEATURE_DIM      = 64    \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def save_unitary(U: np.ndarray, idx: int):\n",
    "    np.save(USER_OUT_DIR / f\"unitary_{idx:06d}.npy\", U)\n",
    "    print(f\"Saved unitary_{idx:06d}.npy\")\n",
    "\n",
    "\n",
    "class UnitaryModel(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.randn(dim, dim, dtype=torch.float64, device=device))\n",
    "\n",
    "    def forward(self):\n",
    "        # project A  U via QR + sign correction\n",
    "        Q, R = torch.linalg.qr(self.A)\n",
    "        diag = torch.sign(torch.diag(R))\n",
    "        U = Q @ torch.diag(diag)\n",
    "        return U\n",
    "\n",
    "def train():\n",
    "    X = torch.zeros(50000,64,dtype=torch.float64)\n",
    "    X[:,0] = 1\n",
    "    Y = torch.tensor(input_state_vectors,dtype=torch.float64)\n",
    "    N = X.size(0)\n",
    "    mse = nn.MSELoss()\n",
    "    X = X[:10]\n",
    "    Y = Y[:10]\n",
    "    global_idx = 0\n",
    "    for start in range(0, N, BATCH_SIZE):\n",
    "        end = min(start + BATCH_SIZE, N)\n",
    "        Xb, Yb, Lb = X[start:end], Y[start:end], labels[start:end]\n",
    "\n",
    "        for local_j in range(Xb.size(0)):\n",
    "            idx = start + local_j\n",
    "            x_i = Xb[local_j:local_j+1]         \n",
    "            y_i = Yb[local_j:local_j+1]           \n",
    "\n",
    "            model = UnitaryModel(FEATURE_DIM).to(device)\n",
    "            optim_ = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "            for it in range(MAX_ITERS):\n",
    "                optim_.zero_grad()\n",
    "                U = model()                    \n",
    "                pred_i = (U @ x_i.reshape(-1,64,1))      \n",
    "                loss_rec = mse(pred_i.reshape(-1,64), y_i)\n",
    "                loss = loss_rec \n",
    "                loss.backward()\n",
    "                optim_.step()\n",
    "                print('Iterations', it , 'loss',loss)\n",
    "            # finalize\n",
    "            with torch.no_grad():\n",
    "                U_final = model().cpu().numpy()\n",
    "                err = np.linalg.norm(U_final @ U_final.conj().T - np.eye(FEATURE_DIM))\n",
    "                print(f\"#{global_idx:06d}: rec={loss_rec.item():.4e}, unit_err={err:.4e}\")\n",
    "\n",
    "                save_unitary(U_final, global_idx)\n",
    "                global_idx += 1\n",
    "\n",
    "    print(\"Done. Saved all unitaries to:\", USER_OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf298cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total circuits to process: 10\n",
      "\n",
      "======================================================================\n",
      "SAVING QS DECOMPOSED CIRCUIT\n",
      "======================================================================\n",
      "QS decomposed circuit successfully saved to: C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_ComposedQR\\circuits_first_10_Quantum_Circuits.txt\n",
      "File size: 134009 bytes\n",
      "Saved batch 0 (0-9) with 10 circuits.\n",
      "All 10 circuits processed and saved in 75.53 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Quantum Computing Circuit for Quantum Computer Hardware Implementation of the above basic algorithm\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from qiskit import QuantumCircuit, qpy\n",
    "from qiskit.quantum_info import Operator\n",
    "from qiskit.synthesis import qs_decomposition, OneQubitEulerDecomposer, TwoQubitBasisDecomposer\n",
    "from qiskit.circuit.library import CXGate\n",
    "\n",
    "MAPPING_DIR = Path(r\"C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_Conjugate_Gradients\")\n",
    "CLASSIFIER_PATH = r\"C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_\\UnitaryFinalClassifierQR.npy\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\Akshay Patil\\Desktop\\Unitary_Batches_Mapping_Input_Data_Train_ComposedQR\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "def get_sorted_npy_files(directory):\n",
    "    files = glob.glob(os.path.join(directory, \"unitary_*.npy\"))\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def load_classifier_matrix(path):\n",
    "    matrix = np.load(path)\n",
    "    if matrix.shape != (64, 64):\n",
    "        raise ValueError(f\"Classifier matrix must be 64x64, got {matrix.shape}\")\n",
    "    return matrix\n",
    "\n",
    "def verify_unitary(matrix, tol=1e-14):\n",
    "    return np.allclose(matrix @ matrix.conj().T, np.eye(64), atol=tol)\n",
    "\n",
    "def compose_and_decompose(mapping_matrix, classifier_matrix):\n",
    "    # Compose: U(Mapping_i)  U(Classifier)\n",
    "    composed = mapping_matrix @ classifier_matrix\n",
    "    if not verify_unitary(composed):\n",
    "        # SVD normalization for numerical stability\n",
    "        u, _, vh = np.linalg.svd(composed)\n",
    "        composed = u @ vh\n",
    "    # QS decomposition\n",
    "    oneq = OneQubitEulerDecomposer(basis='U3')\n",
    "    twoq = TwoQubitBasisDecomposer(CXGate())\n",
    "    circuit = qs_decomposition(\n",
    "        composed,\n",
    "        opt_a1=True,\n",
    "        opt_a2=True,\n",
    "        decomposer_1q=oneq,\n",
    "        decomposer_2q=twoq\n",
    "    )\n",
    "    return circuit\n",
    "\n",
    "\n",
    "\n",
    "def save_qs_decomposed_circuit(circuits, output_path,batch_idx):\n",
    "    \"\"\"Save the QS decomposed circuit to file.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SAVING QS DECOMPOSED CIRCUIT\")\n",
    "    print(\"=\" * 70)\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"QUANTUM CIRCUIT DECOMPOSITION USING QS (QUANTUM SHANNON) DECOMPOSITION\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"Method: Quantum Shannon Decomposition with optimizations\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "            for circuit in circuits:\n",
    "                # Write circuit information\n",
    "                f.write(\"QS DECOMPOSED CIRCUIT INFORMATION:\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                f.write(f\"Circuit name: {circuit.name}\\n\")\n",
    "                f.write(f\"Number of qubits: {circuit.num_qubits}\\n\")\n",
    "                f.write(f\"Total gates: {circuit.size()}\\n\")\n",
    "                f.write(f\"Circuit depth: {circuit.depth()}\\n\\n\")\n",
    "                \n",
    "                # Write gate counts\n",
    "                gate_counts = {}\n",
    "                for instruction in circuit.data:\n",
    "                    gate_name = instruction.operation.name\n",
    "                    gate_counts[gate_name] = gate_counts.get(gate_name, 0) + 1\n",
    "                \n",
    "                f.write(\"GATE COUNTS:\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for gate, count in sorted(gate_counts.items()):\n",
    "                    f.write(f\"{gate}: {count}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "                # Write circuit diagram (limited for readability)\n",
    "                f.write(\"CIRCUIT DIAGRAM (first 20 lines):\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                circuit_str = str(circuit.draw(output='text', fold=100))\n",
    "                lines = circuit_str.split('\\n')\n",
    "                for line in lines[:20]:\n",
    "                    f.write(line + \"\\n\")\n",
    "                if len(lines) > 20:\n",
    "                    f.write(f\"... and {len(lines) - 20} more lines\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "                # Write detailed gate sequence (limited to first 100 gates for readability)\n",
    "                f.write(\"DETAILED GATE SEQUENCE (First 100 gates):\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for i, instruction in enumerate(circuit.data[:100]):\n",
    "                    gate = instruction.operation\n",
    "                    qubits = [circuit.find_bit(q)[0] for q in instruction.qubits]\n",
    "                    \n",
    "                    if hasattr(gate, 'params') and gate.params:\n",
    "                        params_str = f\"({', '.join(f'{p:.6f}' for p in gate.params)})\"\n",
    "                    else:\n",
    "                        params_str = \"\"\n",
    "                    \n",
    "                    f.write(f\"{i+1:4d}: {gate.name}{params_str} on qubits {qubits}\\n\")\n",
    "                \n",
    "                if len(circuit.data) > 100:\n",
    "                    f.write(f\"... and {len(circuit.data) - 100} more gates\\n\")\n",
    "                \n",
    "                f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "                f.write(\"QS DECOMPOSITION COMPLETED SUCCESSFULLY\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\")\n",
    "            \n",
    "        print(f\"QS decomposed circuit successfully saved to: {output_path}\")\n",
    "        print(f\"File size: {os.path.getsize(output_path)} bytes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving circuit: {e}\")\n",
    "\n",
    "\n",
    "def process_batch(mapping_files,classifier_matrix, batch_start, batch_end, output_dir):\n",
    "    circuits = []\n",
    "    for i in range(batch_start,batch_end): \n",
    "        mapping_matrix = np.load(mapping_files[i])\n",
    "        circuit = compose_and_decompose(mapping_matrix, classifier_matrix)\n",
    "        circuit.name = f\"circuit_{i:05d}\"\n",
    "        circuits.append(circuit)\n",
    "    # output_path = output_dir  + f\"circuits_batch_{batch_start // BATCH_SIZE:05d}.txt\"\n",
    "    output_path = output_dir  + f\"\\circuits_first_10_Quantum_Circuits.txt\"\n",
    "    save_qs_decomposed_circuit(circuits, output_path,batch_start // BATCH_SIZE)\n",
    "    print(f\"Saved batch {batch_start // BATCH_SIZE} ({batch_start}-{batch_end-1}) with {len(circuits)} circuits.\")\n",
    "\n",
    "def main():\n",
    "    mapping_files = get_sorted_npy_files(MAPPING_DIR)\n",
    "    classifier_matrix = load_classifier_matrix(CLASSIFIER_PATH)\n",
    "    total_circuits = len(mapping_files)\n",
    "    print(f\"Total circuits to process: {total_circuits}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    # for batch_start in range(0, total_circuits, BATCH_SIZE):\n",
    "    #     batch_end = min(batch_start + BATCH_SIZE, total_circuits)\n",
    "    process_batch(mapping_files, classifier_matrix, 0, 10, OUTPUT_DIR)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"All {total_circuits} circuits processed and saved in {elapsed:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be822b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7582d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
